{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6713ce2f",
   "metadata": {},
   "source": [
    "# Import all used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb609c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:26:09.644433Z",
     "start_time": "2023-06-03T07:26:00.882330Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow .keras.optimizers import Adam\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "#from pandas.tseries.offsets import DateOffset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702b19b",
   "metadata": {},
   "source": [
    "# Load in data in respective periods\n",
    "This is done to alleviate the issue of our machines running out of memory when running code, thus only load in data that is needed for respective period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3db3c",
   "metadata": {},
   "source": [
    "### 2005-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5511d43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T09:06:48.733475Z",
     "start_time": "2023-06-01T09:06:16.185210Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load( open( \"save_X.p\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"save_y.p\", \"rb\" ) )\n",
    "X_test = pickle.load( open( \"save_X_test.p\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"save_y_test.p\", \"rb\" ) )\n",
    "dates_df_test_pf = pickle.load( open( \"dates_df_test_pf.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbcb91",
   "metadata": {},
   "source": [
    "### 2012-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b290697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:05.173889Z",
     "start_time": "2023-06-02T11:17:22.141084Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load( open( \"save_X_12_18.p\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"save_y_12_18.p\", \"rb\" ) )\n",
    "X_test = pickle.load( open( \"save_X_test_12_18.p\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"save_y_test_12_18.p\", \"rb\" ) )\n",
    "dates_df_test_pf = pickle.load( open( \"dates_df_test_pf_12_18.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71667c",
   "metadata": {},
   "source": [
    "### 2019-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aed314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T19:20:15.803237Z",
     "start_time": "2023-05-28T19:19:36.300976Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load( open( \"save_X_19_22.p\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"save_y_19_22.p\", \"rb\" ) )\n",
    "X_test = pickle.load( open( \"save_X_test_19_22.p\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"save_y_test_19_22.p\", \"rb\" ) )\n",
    "dates_df_test_pf = pickle.load( open( \"dates_df_test_pf_19_22.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71294d6c",
   "metadata": {},
   "source": [
    "### 1990-2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc11e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:47:56.019705Z",
     "start_time": "2023-05-25T12:47:24.045581Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load( open( \"save_X_1990.p\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"save_y_1990.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c7637",
   "metadata": {},
   "source": [
    "# Load in NASDAQ 100 data\n",
    "Load in Nasdaq 100 data that is used in the portfolio section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "176b044f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:27.838463Z",
     "start_time": "2023-06-02T11:18:27.786417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008280178107935323, 0.013346305965169048, 0.014801658109719847]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we calculate the average monthly return for each period we are considering. This is used for the rebalancing threshold.\n",
    "Nasdaq100_index = pd.read_csv(\"NASDAQ100_index.csv\")\n",
    "Nasdaq_avg_ret = []\n",
    "for i in range(3):\n",
    "    if i == 0:       \n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2005-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2012-02-01\")][\"NASDAQ100\"].tolist()\n",
    "    elif i == 1:     \n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2012-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2019-02-01\")][\"NASDAQ100\"].tolist()\n",
    "    elif i == 2:\n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2019-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2023-02-01\")][\"NASDAQ100\"].tolist()\n",
    "    # The data has \".\" in a lot of places, that we remove\n",
    "    for x in plot_nas[:]:\n",
    "        if x == '.':\n",
    "            plot_nas.remove(x) \n",
    "    # The dataformat is also strings, so we convert to float\n",
    "    plot_nas = [float(x) for x in plot_nas]\n",
    "    # Calculate returns\n",
    "    ret_nas = []\n",
    "    for i in range(1,len(plot_nas)):\n",
    "        ret_nas.append(plot_nas[i]/plot_nas[i-1]-1)\n",
    "    Nasdaq_avg_ret.append(sum(ret_nas)/np.ceil(len(ret_nas)/21)) # Calculate average return in period\n",
    "\n",
    "\n",
    "Nasdaq_avg_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34142383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T07:27:28.099129Z",
     "start_time": "2023-06-03T07:26:18.802256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c78a1b55a04ff19fee283aaa3b3f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1141398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sorted_dates.csv\") # Load in unfinished dataset\n",
    "\n",
    "df_vix = pd.read_csv(\"VIXCLS.csv\") # Load in VIX (we decided to add it late)\n",
    "# Make manipulations to VIX df and merge it to dataset\n",
    "df_vix[\"datadate\"] = df_vix[\"DATE\"]\n",
    "df_vix[\"datadate\"] = pd.to_datetime(df_vix[\"datadate\"])\n",
    "df[\"datadate\"] = pd.to_datetime(df[\"datadate\"])\n",
    "df_vix = df_vix.drop([\"DATE\"],axis = 1)\n",
    "df = pd.merge_asof(df,df_vix, on =\"datadate\")\n",
    "\n",
    "df_close = pd.read_csv(\"Closing_price_month.csv\") # Load in closing price monthly (we noticed we might need it)\n",
    "df_close[\"datadate\"] = pd.to_datetime(df_close[\"datadate\"])\n",
    "df_close = df_close[[\"tic\",\"datadate\",\"prccm\"]]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = df.fillna(0) #Fill NaN with 0\n",
    "# Remove a bunch of unused columns from dataset\n",
    "df = df.loc[:, ~df.columns.isin([\"return quarterly\",\"tic.1\",\"ggroup.1\", 'ggroup', \"gsector\",\"gsector.1\",\"naics.1\",\"naics\",\"sic.1\",\"sic\"])]\n",
    "df = df.sort_values([\"tic\",\"datadate\"]) # Sort dataset on tickers and datadate, to get in correct order\n",
    "df = pd.merge(df, df_close, on=['tic', 'datadate']) # Merge closing price to dataset\n",
    "# Remove two firms with wildly inconsistent price and return data\n",
    "df = df[df[\"tic\"]!=\"CRGE\"]\n",
    "df = df[df[\"tic\"]!=\"HYMC\"]\n",
    "\n",
    "df[\"log_ret\"] = np.log(df[\"prccm\"]/df[\"prccm\"].shift(1)) # Include log-return from \"current\" month as feature\n",
    "df[\"target\"] = np.log(df[\"prccm\"].shift(-1)/df[\"prccm\"]) # We want to \"forecast\" (predict next month's log-return)\n",
    "\n",
    "# Load in monthly dataset since we need some information from here for the portfolio stuff\n",
    "monthly = pd.read_csv(\"MONTHLY ALL RAW.csv\")\n",
    "monthly = monthly[[\"tic\",\"datadate\",\"prccm\",\"cshom\"]] # Only interested in ticker, datadate, closing price, and shares outstanding\n",
    "monthly[\"mkt cap\"] = monthly[\"prccm\"] * monthly[\"cshom\"] # Calculate market cap (used in market cap weighted portfolios)\n",
    "monthly = monthly.drop([\"prccm\",\"cshom\"],axis=1) # After market cap is calculated, we have no need for closing price and shares outstanding\n",
    "# Make manipulations and merge to dataset\n",
    "monthly[\"datadate\"] = pd.to_datetime(monthly[\"datadate\"]) \n",
    "df = pd.merge(df, monthly, on=['tic', 'datadate'])\n",
    "df[\"mkt cap\"] = df[\"mkt cap\"].fillna(0) # We noticed a few places with NaN values in market cap, remove these\n",
    "\n",
    "\n",
    "# The next few lines correct an issue we had, where, when calculating log-returns, some firms' final datapoints would use\n",
    "# the incorrect data from another firm. This is obviously not correct, so we remove every firm's final datapoint\n",
    "df = df.reset_index(drop = True)\n",
    "temp = [0] # Create temp for storing index\n",
    "for i in tqdm_notebook(range(len(df)-1)):\n",
    "    if df[\"tic\"].iloc[i] != df[\"tic\"].iloc[i+1]: # Find where we need to delete a datapoint (where NaN should have appead)\n",
    "        temp.append(i) # Append the index\n",
    "        temp.append(i+1)\n",
    "temp.append(len(df)-1)\n",
    "df = df.drop(temp) # Remove the index from df\n",
    "df = df.dropna() # Remove NaN values\n",
    "df = df.reset_index(drop = True)\n",
    "df = df.sort_values([\"tic\",\"datadate\"]) # Sort dataset on tickers and datadate, to get in correct order, again\n",
    "\n",
    "n_features = 31 # Choose amount of features for use in later function. We have 31\n",
    "\n",
    "input_size = 12 # Determine the amount of data for each firm that should på put in the model. We choose 12 months\n",
    "train_size = 0.8 # NOT USED!!! - old variable from when we trained on 80% and tested on 20% of data\n",
    "\n",
    "df = df[df[\"datadate\"]<\"2023\"] # We decided for simplicity to cut off the data at 2023\n",
    "\n",
    "# The next lines are what separates the data in the aforementioned periods\n",
    "#df = df[(df[\"datadate\"]<\"2012-01-01\") & (df[\"datadate\"]>\"2004-02-01\")] \n",
    "#df = df[(df[\"datadate\"]<\"2019-01-01\") & (df[\"datadate\"]>\"2011-02-01\")] \n",
    "#df = df[(df[\"datadate\"]<\"2023-01-01\") & (df[\"datadate\"]>\"2018-02-01\")] \n",
    "\n",
    "df = df[(df[\"datadate\"]<\"2005\") & (df[\"datadate\"]>\"1989\")] \n",
    "\n",
    "# After examining descriptor correlation, these were deemed not important, and thus dropped\n",
    "df = df.drop([\"MIDREV\",\"MIDREV excess\",\"STOQ\",\"ATO\",\"TB3MS\",\"rf\"],axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75433b6",
   "metadata": {},
   "source": [
    "Since we decided that we wanted to compare our portfolio results to the Nasdaq 100 index, it makes sense that we build our portfolio on the very same Nasdaq 100 constituents. Thus, we create a test set only containing the firms available in our dataset that were/are Nasdaq constituents in the correct time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1f7b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:09:47.670047Z",
     "start_time": "2023-05-25T07:09:46.832341Z"
    }
   },
   "outputs": [],
   "source": [
    "#create test train set\n",
    "from functools import reduce\n",
    "N100 = pd.ExcelFile(\"Ticker NASDAQ100 data BLOOMBERG.xlsx\") # Load in Nasdaq 100 constituents for every year between 2005 and 2023\n",
    "Nasdaq100 = {} # Create dictionary that will contain the constituents for each year\n",
    "# Divide out information from each excel sheet in dictionary \n",
    "years = np.arange(2004,2024)\n",
    "Tickers = []\n",
    "for year in years:\n",
    "    Nasdaq100[year] = pd.read_excel(N100, str(year))\n",
    "    for j in range(len(Nasdaq100[year])):\n",
    "        Nasdaq100[year].at[j,\"Ticker\"] = Nasdaq100[year][\"Ticker\"].tolist()[j].split()[0]\n",
    "    Tickers = reduce(np.union1d, (Tickers,Nasdaq100[year][\"Ticker\"])) # End up with complete list of Nasdaq 100 constituents\n",
    "    \n",
    "        \n",
    "unique_tics = np.unique(df[\"tic\"]) # Determine the unique tickers in dataset\n",
    "intersect = np.intersect1d(unique_tics,Tickers) # Find the intersection between tickers in dataset and Nasdaq 100 constituents \n",
    "train = df[~df[\"tic\"].isin(intersect)] # Create train set with every ticker NOT in Nasdaq 100\n",
    "test = df[df[\"tic\"].isin(intersect)] # Create test set with every ticker in Nasdaq 100\n",
    "test_pf = df[df[\"tic\"].isin(intersect)] # Create duplicate test set with every ticker in Nasdaq 100 for portfolio stuff\n",
    "train = train.drop([\"prccm\",\"mkt cap\"],axis = 1) # Remove close price and market cap, since these are not used for training \n",
    "test = test.drop([\"prccm\",\"mkt cap\"],axis = 1) # Remove close price and market cap, since these are not used for testing\n",
    "test_pf = test_pf[[\"datadate\",\"tic\",\"prccm\",\"mkt cap\"]] # Only keep datadate, ticker, close price, and market cap in duplicate test set used for portfolio stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eee12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:09:51.692458Z",
     "start_time": "2023-05-25T07:09:49.290580Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale both train and test set\n",
    "for col in train.columns[2:-1]:\n",
    "    scaler = MinMaxScaler()\n",
    "    train[[col]] = scaler.fit_transform(train[[col]]) # Use fit_transform on training set\n",
    "    test[[col]] = scaler.transform(test[[col]]) # Use transform on test set. ONLY NASDAQ COMPANIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd2754c",
   "metadata": {},
   "source": [
    "The next functions are what uses the input_size parameter from earlier. Basically, we want to represent the data in \"chunks\" of _input_size_ (this case 12) datapoints for each firm at a time. Thus, the first function creates a list, _dates_df_, of dataframes for the train set each containing a years worth of data with the next index of the list sliding one month ahead. The final output is as said a list where the index is the corresponding month's index (dates_df[0] contains the first month and so on)\n",
    "\n",
    "The bottom function does roughly the same but for the test set. A key difference is, that the resulting list is a level deeper since we need to keep track of specific tickers for each year. Thus the first index corresponds to the year and the next index is the month (dates_df_test[0][0] will be first month of first year, while dates_df_test[1][11] is the last month of the next year and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb88b8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:10:09.357790Z",
     "start_time": "2023-05-25T07:10:07.460539Z"
    }
   },
   "outputs": [],
   "source": [
    "def input_size_func(df):\n",
    "    dates_df = [] # Allocate storage to save data for each month\n",
    "    unique_dates = np.unique(df[\"datadate\"]) # Determine the unique dates \n",
    "    for i in range(input_size-1,len(unique_dates)): # Iterate over every month but start at \"input_size\"-1 since we cant look 12 months back starting at month 0\n",
    "        try:\n",
    "            # Look at data in range of \"input_size\" sliding one month ahead at a time\n",
    "            dates_df.append(df[(df[\"datadate\"]>=unique_dates[i-input_size+1]) & (df[\"datadate\"]<=unique_dates[i])].reset_index(drop=True))\n",
    "        except:\n",
    "            None\n",
    "    return dates_df\n",
    "\n",
    "def input_size_func_test(dictionary,test_set):\n",
    "    input_size_test = [] # Allocate storage to save data for each month\n",
    "    years = np.arange(2019,2023) #'''REMEMBER TO CHANGE!!!''' # Decide which years to iterate over (three respective periods)\n",
    "    for year in years:\n",
    "        temp=test_set[test_set[\"tic\"].isin(np.unique(dictionary[year][\"Ticker\"]))] # Look at specific tickers every year        \n",
    "        temp_test=temp[(temp[\"datadate\"]>=str(year-1)+\"-02-01\") & (temp[\"datadate\"]<str(year+1) )] # Look at data in range of \"input_size\" sliding one month ahead at a time\n",
    "        input_size_test.append(input_size_func(temp_test)) # Use above function for each year, creating a list that will be 1 level deeper\n",
    "    return input_size_test\n",
    "        \n",
    "dates_df_train = input_size_func(train)    \n",
    "dates_df_test_dict = input_size_func_test(Nasdaq100,test)\n",
    "dates_df_test_pf = input_size_func_test(Nasdaq100,test_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592af3a7",
   "metadata": {},
   "source": [
    "We noticed a little too late on, that the way we created the _input_size_ data in the above functions, we did not make sure that we only had data in chunks of 12. Thus, some places would contain a chunk of some random number of data points. This created a bunch of issues, since we need chunks of 12. A quick fix was to create two functions that could remove the inconsistensies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c38fba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:21:58.487528Z",
     "start_time": "2023-05-25T07:10:12.671252Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_input_size_errors_train(dates_df_train):\n",
    "    for l in tqdm_notebook(range(len(dates_df_train))): # Iterate over every index of list \n",
    "        k=0 # Assign start-index variable, so we can start next loop from index where last item is removed\n",
    "        stop = 1 # Assign dummy variable to determine if we have removed something and can stop the current list index \n",
    "        # (stop == 0 means that we have not stopped in an entire run-through, and can then jump to next index) \n",
    "        for j in range(99999999):\n",
    "            if stop == 0: # If we have not stopped in the previous run-through of current index there are no inconsitensies, we can jump to next index\n",
    "                break\n",
    "            stop = 0 # Set stop variable\n",
    "            for i in range(k,len(dates_df_train[l]),input_size): # Jump 12 each step, start at k so we dont have to start from beginning when removing \n",
    "                try:\n",
    "                    if dates_df_train[l].iloc[i][\"tic\"]!=dates_df_train[l].iloc[i+input_size-1][\"tic\"]: # If the firm 11 places in front is different we dont have chunk of 12\n",
    "                        dates_df_train[l] = dates_df_train[l].drop(i).reset_index(drop=True) # Remove current index and reset index\n",
    "                        k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                        stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "                        break\n",
    "                except:\n",
    "                    dates_df_train[l] = dates_df_train[l].drop(i).reset_index(drop=True) # We will end up in except statement near the end if there is inconsistency. Remove this\n",
    "                    k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                    stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "    return dates_df_train\n",
    "\n",
    "def remove_input_size_errors_test(dates_df_test):\n",
    "    # Remember that this list is a level deeper, so we need to keep track of both year and month\n",
    "    year = 0\n",
    "    month = 0\n",
    "    for l in tqdm_notebook(range(len(dates_df_test)*12)): # The total number of indexes is the amount of years multiplied with 12\n",
    "        k=0 # Assign start-index variable, so we can start next loop from index where last item is removed\n",
    "        stop = 1 # Assign dummy variable to determine if we have removed something and can stop the current list index\n",
    "        for j in range(9999999):\n",
    "            if stop == 0: # If we have not stopped in the previous run-through of current index there are no inconsitensies, we can jump to next index\n",
    "                break\n",
    "            stop = 0 # Set stop variable\n",
    "            for i in range(k,len(dates_df_test[year][month]),input_size): # Jump 12 each step, start at k so we dont have to start from beginning when removing \n",
    "                try:\n",
    "                    if dates_df_test[year][month].iloc[i][\"tic\"]!=dates_df_test[year][month].iloc[i+input_size-1][\"tic\"]: # If the firm 11 places in front is different we dont have chunk of 12\n",
    "                        dates_df_test[year][month] = dates_df_test[year][month].drop(i).reset_index(drop=True) # Remove current index and reset index\n",
    "                        k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                        stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "                        break\n",
    "                except:\n",
    "                    dates_df_test[year][month] = dates_df_test[year][month].drop(i).reset_index(drop=True) # We will end up in except statement near the end if there is inconsistency. Remove this\n",
    "                    k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                    stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "        month = (month+1) % 12 # Update month (every time we reach 12 it will reset to 0 and begin a new year)\n",
    "        year = int(np.floor((l+1)/12)) # Update year, only change when we reach 12th, 24th, ... index\n",
    "    return dates_df_test\n",
    "\n",
    "dates_df_train = remove_input_size_errors_train(dates_df_train)\n",
    "dates_df_test_dict = remove_input_size_errors_test(dates_df_test_dict)\n",
    "dates_df_test_pf = remove_input_size_errors_test(dates_df_test_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d257d67",
   "metadata": {},
   "source": [
    "The next functions split datasets into features and target, but also keeps individual firms separated. We want the data in a way where 12 months of features correspond to the last month's target. Example: We save features for one firm from Jan to Dec, and save the target for Dec.\n",
    "\n",
    "Again, bottom function is a level deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b85044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:26:04.286313Z",
     "start_time": "2023-05-25T07:23:11.083404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split(dictionary):\n",
    "    # Create dictionaries that will contain features and target \n",
    "    X = {}\n",
    "    y = {}\n",
    "    for i in tqdm_notebook(range(len(dictionary))):\n",
    "        # Create temporary lists that will contain dataframes containg chunks of 12 for each individual firm, so 1 dataframe per firm\n",
    "        save_X = []\n",
    "        save_y = []\n",
    "        try:\n",
    "            for j in range(len(dictionary[i])):\n",
    "                # If next firm is different, we need to save the features from previous 11 months and from current month + target from current month\n",
    "                if dictionary[i][\"tic\"].iloc[j] != dictionary[i][\"tic\"].iloc[j+1]: \n",
    "                    save_X.append(dictionary[i].iloc[j-input_size+1:j+1,2:-1]) # Save features \n",
    "                    save_y.append(dictionary[i].iloc[j,-1]) # Save target\n",
    "                    \n",
    "        except: # We end up in the except statement in the end\n",
    "            save_X.append(dictionary[i].iloc[j-input_size+1:j+1,2:-1]) # Save features \n",
    "            save_y.append(dictionary[i].iloc[j,-1]) # Save target\n",
    "        #Save temporary lists in dictionary\n",
    "        X[i] = save_X \n",
    "        y[i] = save_y\n",
    "    return X,y\n",
    "\n",
    "def split_test(test_dict):\n",
    "    # Create dictionaries that will contain features and target \n",
    "    X_temp,y_temp = {},{}\n",
    "    for k in range(len(test_dict)):\n",
    "        X_,y_ = split(test_dict[k]) # Use above function for every year of test set\n",
    "        X_temp[k] = X_ # Save features for each year in dictionary\n",
    "        y_temp[k] = y_ # Save target for each year in dictionary\n",
    "    return X_temp,y_temp\n",
    "\n",
    "X_train, y_train = split(dates_df_train)\n",
    "# X_test, y_test = split_test(dates_df_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a993c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:26:37.858477Z",
     "start_time": "2023-05-25T07:26:22.416832Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump( X_train, open( \"save_X_1990.p\", \"wb\" ) )\n",
    "pickle.dump( y_train, open( \"save_y_1990.p\", \"wb\" ) )\n",
    "# pickle.dump( X_test, open( \"save_X_test_19_22.p\", \"wb\" ) )\n",
    "# pickle.dump( y_test, open( \"save_y_test_19_22.p\", \"wb\" ) )\n",
    "# pickle.dump( dates_df_test_pf, open( \"dates_df_test_pf_19_22.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ee4ea",
   "metadata": {},
   "source": [
    "# Run code from here \n",
    "We are now done with data manipulation, and ready to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45a7a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:05.189914Z",
     "start_time": "2023-06-02T11:18:05.175891Z"
    }
   },
   "outputs": [],
   "source": [
    "# If model is not run from beginning, but pickled data is loaded in, run this to assign varaibles\n",
    "input_size = 12\n",
    "n_features = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed93296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:47:56.051734Z",
     "start_time": "2023-05-25T12:47:56.038723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create RNN model that we use in project\n",
    "def create_model_rnn(neurons,learning_rate,drop_out,hidden):\n",
    "    early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50) # Stop if no progress in # epochs\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.best.hdf5\",monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [early] #,checkpoint\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    model.add(Dropout(rate=drop_out))\n",
    "    \n",
    "    # Create functionality for using different amounts of hidden layers used in grid search\n",
    "    if hidden == 1:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    elif  hidden == 2:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    elif  hidden == 3:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        \n",
    "    model.add(SimpleRNN(units = neurons, activation = 'relu', return_sequences=False ))\n",
    "    model.add(Dense(units = 1)) #Linear output layer\n",
    "    opt = optimizers.Adam(lr=learning_rate, clipnorm=1.)  \n",
    "    model.compile(optimizer = opt, loss = \"mse\")\n",
    "    return model,callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422282a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:47:56.067748Z",
     "start_time": "2023-05-25T12:47:56.053736Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "neurons = [50]\n",
    "epochs = [50]\n",
    "learning_rate = [0.0005,0.001] \n",
    "drop_out = [0.05,0.01]\n",
    "batch_size = [32,64]\n",
    "hidden = [1,2]\n",
    "val_size = 0.2\n",
    "loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4150b",
   "metadata": {},
   "source": [
    "Below, we do the grid search. We run a model for every combination of hyperparameters given above and sort the different models on their validation mse.\n",
    "\n",
    "Bottom function makes it possible to run big grid on firstt month and then use optimal hyperparameters on next 2 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f894f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:47:56.099777Z",
     "start_time": "2023-05-25T12:47:56.069750Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_model(X_train,y_train,starting_point,end_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out):\n",
    "    optimal = []\n",
    "    if len(neurons) == 1 & len(epochs) == 1 & len(learning_rate) == 1 & len(drop_out) == 1 & len(batch_size) == 1 & starting_point == end_point:\n",
    "        verbose = 1\n",
    "    else:\n",
    "        verbose = 0\n",
    "    for i in tqdm_notebook(range(starting_point,end_point+1)):\n",
    "        print(i)\n",
    "        print(\"---\")\n",
    "        temp = []\n",
    "        for j in range(len(neurons)):\n",
    "            for q in range(len(epochs)):\n",
    "                for k in range(len(learning_rate)):\n",
    "                    for p in range(len(drop_out)):\n",
    "                        for l in range(len(batch_size)):\n",
    "                            for h in range(len(hidden)):\n",
    "                                print(\"Neurons:\",neurons[j],\". Epochs:\",epochs[q],\". Learning rate:\",learning_rate[k],\". Dropout:\",drop_out[p],\". Batch size:\",batch_size[l], \". Hidden layers:\", hidden[h])\n",
    "                                # Create model using function above\n",
    "                                model,callbacks_list = create_model_rnn(neurons[j],learning_rate[k],drop_out[p],hidden[h])\n",
    "                                # Fit model on training data, save loss history\n",
    "                                hist = model.fit(np.array(X_train[i]),np.array(y_train[i]),epochs=epochs[q], verbose=verbose,callbacks=callbacks_list,validation_split=0.2,batch_size = batch_size[l]) \n",
    "                                # Predict data using train as input to calculate train MSE\n",
    "                                pred_train = model.predict(np.array(X_train[i]))\n",
    "                                # Predict data using validation data as input to calculate validation MSE\n",
    "                                pred_val = model.predict(np.array(X_train[i])[int(len(np.array(X_train[i]))*0.8):])\n",
    "                                # Save dataframe of different MSEs, hyperparameters used, models, and loss results\n",
    "                                temp.append([mean_squared_error(np.array(y_train[i]),pred_train),mean_squared_error(np.array(y_train[i])[int(len(y_train[i])*0.8):],pred_val),neurons[j],epochs[q],learning_rate[k],drop_out[p],batch_size[l],hidden[h],model,hist.history])\n",
    "        df_temp = pd.DataFrame(data = temp,columns = (\"train mse\",\"val mse\",\"neurons\",\"epochs\",\"learning_rate\",\"dropout\",\"batch size\",\"hidden layers\",\"model_save\",\"loss_hist\"))\n",
    "        optimal.append(df_temp)\n",
    "    return optimal, starting_point\n",
    "\n",
    "def run_multiple(X_train,y_train,starting_point,end_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out):\n",
    "    optimal_1, index_start = find_optimal_model(X_train,y_train,starting_point,starting_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "    temp_hyper = optimal_1[0][optimal_1[0][\"val mse\"]==(optimal_1[0][\"val mse\"].min())]\n",
    "    optimal_2, start = find_optimal_model(X_train,y_train,starting_point+1,end_point,[temp_hyper.iloc[0][\"epochs\"]],[temp_hyper.iloc[0][\"neurons\"]],[temp_hyper.iloc[0][\"learning_rate\"]],[temp_hyper.iloc[0][\"batch size\"]],[temp_hyper.iloc[0][\"hidden layers\"]],[temp_hyper.iloc[0][\"dropout\"]])\n",
    "    return optimal_1+optimal_2,index_start, end_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a471ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T13:05:40.682251Z",
     "start_time": "2023-05-25T12:47:56.101779Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run above functions to find models, specify start and end index\n",
    "start = 78\n",
    "ending = 83 \n",
    "optimal1,index_start,index_end = run_multiple(X_train,y_train,start,start+2,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "optimal2,index_start,slut = run_multiple(X_train,y_train,index_end+1,ending,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "optimal = optimal1+optimal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f7d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T13:05:52.181135Z",
     "start_time": "2023-05-25T13:05:52.157113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find every optimal model and save in dataframe along with hyperparameters used\n",
    "def choose_optimal(optimal,index_start):\n",
    "    df = pd.DataFrame()\n",
    "    year = int(np.floor(index_start/12))\n",
    "    month = index_start % 12\n",
    "    for i in range(len(optimal)):\n",
    "        temp = optimal[i][optimal[i][\"val mse\"]==(optimal[i][\"val mse\"].min())]\n",
    "        df = df.append(temp)\n",
    "        print(year,month)\n",
    "        month = month+1\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "optimal_df = choose_optimal(optimal,start)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e5c02b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T18:01:10.459473Z",
     "start_time": "2023-05-23T18:01:10.304332Z"
    }
   },
   "source": [
    "def choose_optimal_and_predict(optimal,X_test,index_start):\n",
    "    df = pd.DataFrame()\n",
    "    year = int(np.floor(index_start/12))\n",
    "    month = index_start % 12\n",
    "    \n",
    "    for i in range(len(optimal)):\n",
    "        temp = optimal[i][optimal[i][\"val mse\"]==(optimal[i][\"val mse\"].min())]\n",
    "        \n",
    "#         if i %12 == 0:\n",
    "#             year = year+1\n",
    "#             month = 0 #reset month if we change year\n",
    "        print(year,month)\n",
    "        temp_pred = temp.iloc[0][8].predict(np.array(X_test[year][month])) #[0][8] chooses the model in this line.\n",
    "        temp[\"pred\"] = [temp_pred]\n",
    "        df = df.append(temp)\n",
    "        month = month+1\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "optimal_df = choose_optimal_and_predict(optimal,X_test,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7edc2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T11:31:23.978412Z",
     "start_time": "2023-05-25T11:31:23.939113Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e7653",
   "metadata": {},
   "source": [
    "# Pickle data here when done running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f358c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T13:06:04.615249Z",
     "start_time": "2023-05-25T13:06:04.603237Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump( optimal_df.drop([\"model_save\"],axis=1), open( \"optimal_df_1997_2-2.p\", \"wb\" ) ) # Remember to change name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98364e77",
   "metadata": {},
   "source": [
    "# Load in and gather saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f950939d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:30:03.473034Z",
     "start_time": "2023-05-31T07:30:03.303620Z"
    }
   },
   "outputs": [],
   "source": [
    "y1997_ = pickle.load( open( \"optimal_df_1997_2-2.p\", \"rb\" ) )\n",
    "y1998 = pickle.load( open( \"optimal_df_1998_1-2.p\", \"rb\" ) )\n",
    "y1998_ = pickle.load( open( \"optimal_df_1998_2-2.p\", \"rb\" ) )\n",
    "y1999 = pickle.load( open( \"optimal_df_1999_1-2.p\", \"rb\" ) )\n",
    "y1999_ = pickle.load( open( \"optimal_df_1999_2-2.p\", \"rb\" ) )\n",
    "y2000 = pickle.load( open( \"optimal_df_2000_1-2.p\", \"rb\" ) )\n",
    "y2000_ = pickle.load( open( \"optimal_df_2000_2-2.p\", \"rb\" ) )\n",
    "y2001 = pickle.load( open( \"optimal_df_2001_1-2.p\", \"rb\" ) )\n",
    "y2001_ = pickle.load( open( \"optimal_df_2001_2-2.p\", \"rb\" ) )\n",
    "y2002 = pickle.load( open( \"optimal_df_2002_1-2.p\", \"rb\" ) )\n",
    "y2002_ = pickle.load( open( \"optimal_df_2002_2-2.p\", \"rb\" ) )\n",
    "y2003 = pickle.load( open( \"optimal_df_2003_1-2.p\", \"rb\" ) )\n",
    "y2003_ = pickle.load( open( \"optimal_df_2003_2-2.p\", \"rb\" ) )\n",
    "y2004 = pickle.load( open( \"optimal_df_2004_1-2.p\", \"rb\" ) )\n",
    "y2004_ = pickle.load( open( \"optimal_df_2004_2-2.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "optimal_df_97_04 = pd.concat([y1997_,y1998,y1998_,y1999,y1999_,y2000,y2000_,y2001,y2001_,y2002,y2002_,y2003,y2003_,y2004,y2004_]).reset_index(drop=True)\n",
    "pickle.dump( optimal_df_97_04, open( \"optimal_df_97-04.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1787653f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:30:06.786405Z",
     "start_time": "2023-05-31T07:30:06.759381Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_df_91_97 = pickle.load( open( \"optimal_df_91-97.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cec3fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:30:54.419441Z",
     "start_time": "2023-05-31T07:30:54.397419Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_df_pre = pd.concat([optimal_df_97_04,optimal_df_91_97])\n",
    "pickle.dump( optimal_df_pre, open( \"optimal_df_91-04.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4768e582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T09:07:34.064618Z",
     "start_time": "2023-06-01T09:07:34.035332Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_df = pickle.load( open( \"2005_FINAL.p\", \"rb\" ) )\n",
    "#pickle.dump( optimal_df, open( \"optimal_df_05-11.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7524631a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:05.298011Z",
     "start_time": "2023-06-02T11:18:05.192907Z"
    }
   },
   "outputs": [],
   "source": [
    "y2012 = pickle.load( open( \"optimal_df_2012_1-2\", \"rb\" ) )\n",
    "y2012_ = pickle.load( open( \"optimal_df_2012_2-2\", \"rb\" ) )\n",
    "y2013 = pickle.load( open( \"optimal_df_2013_1-2\", \"rb\" ) )\n",
    "y2013_ = pickle.load( open( \"optimal_df_2013_2-2\", \"rb\" ) )\n",
    "y2014 = pickle.load( open( \"optimal_df_2014_1-2\", \"rb\" ) )\n",
    "y2014_ = pickle.load( open( \"optimal_df_2014_2-2\", \"rb\" ) )\n",
    "y2015 = pickle.load( open( \"optimal_df_2015_1-2\", \"rb\" ) )\n",
    "y2015_ = pickle.load( open( \"optimal_df_2015_2-2\", \"rb\" ) )\n",
    "y2016 = pickle.load( open( \"optimal_df_2016_1-2\", \"rb\" ) )\n",
    "y2016_ = pickle.load( open( \"optimal_df_2016_2-2\", \"rb\" ) )\n",
    "y2017 = pickle.load( open( \"optimal_df_2017_1-2\", \"rb\" ) )\n",
    "y2017_ = pickle.load( open( \"optimal_df_2017_2-2\", \"rb\" ) )\n",
    "y2018 = pickle.load( open( \"optimal_df_2018_1-2\", \"rb\" ) )\n",
    "y2018_ = pickle.load( open( \"optimal_df_2018_2-2\", \"rb\" ) )\n",
    "\n",
    "\n",
    "optimal_df = pd.concat([y2012,y2012_,y2013,y2013_,y2014,y2014_,y2015,y2015_,y2016,y2016_,y2017,y2017_,y2018,y2018_]).reset_index(drop=True)\n",
    "#pickle.dump( optimal_df, open( \"optimal_df_12-18.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c19425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:32:02.200791Z",
     "start_time": "2023-05-31T07:32:02.126675Z"
    }
   },
   "outputs": [],
   "source": [
    "y2019 = pickle.load( open( \"optimal_df_2019_1-2\", \"rb\" ) )\n",
    "y2019_ = pickle.load( open( \"optimal_df_2019_2-2\", \"rb\" ) )\n",
    "y2020 = pickle.load( open( \"optimal_df_2020_1-2\", \"rb\" ) )\n",
    "y2020_ = pickle.load( open( \"optimal_df_2020_2-2\", \"rb\" ) )\n",
    "y2021 = pickle.load( open( \"optimal_df_2021_1-2\", \"rb\" ) )\n",
    "y2021_ = pickle.load( open( \"optimal_df_2021_2-2\", \"rb\" ) )\n",
    "y2022 = pickle.load( open( \"optimal_df_2022_1-2\", \"rb\" ) )\n",
    "y2022_ = pickle.load( open( \"optimal_df_2022_2-2\", \"rb\" ) )\n",
    "\n",
    "\n",
    "optimal_df = pd.concat([y2019,y2019_,y2020,y2020_,y2021,y2021_,y2022,y2022_]).reset_index(drop=True)\n",
    "pickle.dump( optimal_df, open( \"optimal_df_19-22.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfcdaf",
   "metadata": {},
   "source": [
    "# Run portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe3f0",
   "metadata": {},
   "source": [
    "We are now ready for the portfolio stuff. \n",
    "\n",
    "First function is what goes through every month and manages portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60264abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:07.854329Z",
     "start_time": "2023-06-02T11:18:07.830307Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimal_hyper(optimal_df,X_test,y_test,dates_df_test_pf, frequency):\n",
    "    df = pd.DataFrame(columns = (\"ls_ret_eq\",\"ls_ret_mc\",\"bh_ret_eq\",\"bh_ret_mc\")) # Create df that will save returns from all portfolios each month\n",
    "    # We need to keep track of the current year and month that we are running through. Start is year 0 month 0.\n",
    "    year = -1 # make sure modulus starts at right point, year will be increasesed to 0. This is done beacuse when i=0 we will go into year increase\n",
    "    month = 0\n",
    "    \n",
    "    rebal = 1 # Initialize paramater that keeps track of when we are in a month where we can rebalance\n",
    "    \n",
    "    bh_ret_eq,bh_ret_mc = 0,0 # we have no return from buy/hold in first period\n",
    "    bh_value_eq,bh_value_mc = 100,100 # Start with $100 (We actually ended up not using the value)\n",
    "    bh_pf_eq,bh_pf_mc = pd.DataFrame(),pd.DataFrame() # we have no portfolios in first period\n",
    "    portfolio_stocks = {} # Initialize dictionary that will hold all stocks we use for every month\n",
    "    returns_list = [] # Initialize list that will save returns from every portfolio for every month\n",
    "    \n",
    "    save_i = [] # ALlocate storage to save indexes of rebalance points \n",
    "    for i in range(len(optimal_df)):\n",
    "        # First if-statement keeps track of when we can rebalance\n",
    "        if i %frequency !=0: # If freq is 1 then we rebalance every month. If freq is 3, we can rebalance every third month and so on\n",
    "            bh_ret_eq = 999 # If we are in a month where we can NOT rebalance, we just set the return to a large value, so we do not enter rebalance in make_pf func\n",
    "            bh_ret_mc = 999 \n",
    "            rebal = 0 # Set rebal = 0 when we can not rebalance\n",
    "        elif i%frequency == 0: # In a month we can rebalance, save the index, so we can plot it\n",
    "            save_i.append(i)\n",
    "        \n",
    "        temp = optimal_df.iloc[i] # Save current month's predictions etc. in temp value\n",
    "        \n",
    "        # Update year every 12th index, at the same time set month to 0 for the new year\n",
    "        if i %12 == 0:\n",
    "            year = year+1\n",
    "            month = 0\n",
    "        \n",
    "        # Run select_stocks to save tickers, predicitons, true returns, and market cap for each month\n",
    "        portfolio_stocks[i] = select_stocks(temp[\"pred\"],np.array(y_test[year][month]),dates_df_test_pf,year,month)\n",
    "        # Run make_pf to create/update/manage portfolios for every month - function returns:\n",
    "        # Returns from both long/short pf\n",
    "        # Stocks in both buy/hold pf, returns from both buy/hold, value of both buy/hold\n",
    "        ls_ret_eq, ls_ret_mc, bh_pf_eq, bh_pf_mc, bh_ret_eq, bh_ret_mc, bh_value_eq, bh_value_mc = make_pf(portfolio_stocks[i],bh_ret_eq,bh_ret_mc,bh_value_eq,bh_value_mc,bh_pf_eq,bh_pf_mc,Nasdaq_avg_ret[2],rebal )\n",
    "\n",
    "        df.loc[len(df)] = [ls_ret_eq,ls_ret_mc,bh_ret_eq,bh_ret_mc] # Save returns in df\n",
    "        \n",
    "        returns_list.append([ls_ret_eq,ls_ret_mc,bh_ret_eq,bh_ret_mc]) # Save returns in list (we ended up not using this)\n",
    "        \n",
    "        month = month+1 #increase 1 month\n",
    "        rebal = 1 # Reset rebal for next month\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, portfolio_stocks, returns_list, save_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d12474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:08.554966Z",
     "start_time": "2023-06-02T11:18:08.537951Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_stocks(pred_ret,true_ret,dates_df_test_pf,year,month):\n",
    "    # Function gathers tickers, predicted returns, true returns, and market cap for all stocks\n",
    "    tic_cap = pd.DataFrame() \n",
    "    for l in range(2,len(dates_df_test_pf[year][month]),input_size): # Get tickers from dates_df_test_pf, but only need 1 instance per firm\n",
    "        tic_cap = tic_cap.append(dates_df_test_pf[year][month][l:l+1])\n",
    "    \n",
    "    df = pd.DataFrame(columns = (\"tic\",\"pred\",\"true\",\"market cap\"))\n",
    "    df[\"tic\"] = tic_cap[\"tic\"]\n",
    "    df[\"pred\"] = np.exp(pred_ret)-1 #Transform log-returns to normal returns\n",
    "    df[\"true\"] = np.exp(true_ret)-1 #Transform log-returns to normal returns\n",
    "    df[\"market cap\"] = tic_cap[\"mkt cap\"]\n",
    "    df = df.sort_values(\"pred\",ascending=False) # Sort everything according to predicted returns - highest to lowest\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238471e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:09.064964Z",
     "start_time": "2023-06-02T11:18:09.030933Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_pf(df,bh_ret_eq,bh_ret_mc,bh_value_eq,bh_value_mc,bh_pf_eq,bh_pf_mc,Nasdaq_avg_ret,rebal):\n",
    "    # Function should create 2 portfolios: Long/short & buy/hold\n",
    "    \n",
    "    '''_______________ long/short _______________''' \n",
    "    # First create long/short portfolios that go long in top 10% performing stocks and short in bottom 10% performing stocks\n",
    "    \n",
    "    ls_pf = df.copy().drop(df.index[int(len(df)*0.1):int(len(df)*0.9)]).reset_index(drop=True) # Create df 20% of the length of input stocks\n",
    "\n",
    "    if len(ls_pf) % 2 != 0: # For zero-net investment, we make sure that we long and short same amount of stocks\n",
    "        ls_pf = ls_pf.drop(np.floor(len(ls_pf)/2)).reset_index(drop=True) # Thus, if we have odd amount of stocks, remove middle index\n",
    "    \n",
    "    # Equal weight\n",
    "    ls_pf[\"equal\"] = (ls_pf[\"true\"])*1/(len(ls_pf)/2) # Multiply both long and short positions with 1/(length of long/short)\n",
    "    ls_pf[\"equal\"].iloc[len(ls_pf)//2:] = ls_pf[\"equal\"].iloc[len(ls_pf)//2:]*(-1) # Multiply short positions with minus 1 (get opposite sign returns)\n",
    "    ls_ret_eq = np.sum(ls_pf[\"equal\"]) # The return of the portfolio will be the sum of the returns\n",
    "    \n",
    "    # Market cap weight\n",
    "    # For the market-cap-weighted do the same as above, but this time multiply by weights: mkt_cap/(total mkt_cap of long/short positions)\n",
    "    ls_pf[\"mkt_cap\"] = (ls_pf[\"true\"])\n",
    "    ls_pf[\"mkt_cap\"].iloc[:len(ls_pf)//2] = ls_pf[\"mkt_cap\"].iloc[:len(ls_pf)//2]*1*ls_pf[\"market cap\"].iloc[:len(ls_pf)//2]/sum(ls_pf[\"market cap\"].iloc[:len(ls_pf)//2])\n",
    "    ls_pf[\"mkt_cap\"].iloc[len(ls_pf)//2:] = ls_pf[\"mkt_cap\"].iloc[len(ls_pf)//2:]*(-1)*ls_pf[\"market cap\"].iloc[len(ls_pf)//2:]/sum(ls_pf[\"market cap\"].iloc[len(ls_pf)//2:])\n",
    "    ls_ret_mc = np.sum(ls_pf[\"mkt_cap\"]) # The return of the portfolio will be the sum of the returns\n",
    "    \n",
    "    \n",
    "    '''_______________ buy/hold _______________'''\n",
    "    # Next create buy/hold portfolios that buy stocks with positive predictions and holds, and goes cash when market goes down\n",
    "    \n",
    "    # Equal weight\n",
    "    # Check if we should rebalance or not depending on the return from previous period\n",
    "    if bh_ret_eq >= Nasdaq_avg_ret:\n",
    "        if bh_pf_eq.empty: # If we are holding cash, the return from this month is 0\n",
    "            bh_ret_eq = 0\n",
    "        elif len(df[df[\"pred\"]>0])<1 and rebal == 1: # If we should not rebalance, check if next month will go down\n",
    "            # If yes, and we are able to rebalance, go cash to avoid next month's crash\n",
    "            bh_ret_eq = 0 \n",
    "        else:\n",
    "            # If we are in a situation where we should not rebalance, save current stocks in the portfolio\n",
    "            tics = np.unique(df[\"tic\"])\n",
    "            bh_pf_eq = bh_pf_eq[bh_pf_eq[\"tic\"].isin(tics)].sort_values(\"tic\")\n",
    "            bh_pf_eq[\"true\"] = df[df[\"tic\"].isin(bh_pf_eq[\"tic\"].unique())].sort_values(\"tic\")[\"true\"].tolist()\n",
    "            bh_pf_eq[\"equal\"] = (bh_pf_eq[\"true\"]+1)*bh_value_eq/len(bh_pf_eq) # Calculate weighted return of each stock in pf (equal)\n",
    "            bh_ret_eq = (sum(bh_pf_eq[\"equal\"])/bh_value_eq)-1 # Find total return for month\n",
    "            bh_value_eq = sum(bh_pf_eq[\"equal\"]) # Update total pf value\n",
    "    \n",
    "    # Check if returns are below threshold so we should rebalance (If we are in a month where we can NOT rebalance, we will never enter here)\n",
    "    elif bh_ret_eq < Nasdaq_avg_ret:\n",
    "        bh_pf_eq = df[df[\"pred\"]>0].copy() # Rebalance by buying every stock with positive prediction\n",
    "        if bh_pf_eq.empty: # If all predictions are negative, we go cash\n",
    "            bh_ret_eq = 0\n",
    "        elif len(bh_pf_eq)<1: # Same as above, but can be changed if we want more positive predictions before we buy\n",
    "            bh_ret_eq = 0\n",
    "        else:\n",
    "            bh_pf_eq[\"equal\"] = (bh_pf_eq[\"true\"]+1)*bh_value_eq/len(bh_pf_eq) # Calculate weighted return of each stock in pf (equal)\n",
    "            bh_ret_eq = (sum(bh_pf_eq[\"equal\"])/bh_value_eq)-1 # Find total return for month\n",
    "            bh_value_eq = sum(bh_pf_eq[\"equal\"]) # Update total pf value\n",
    "\n",
    "        \n",
    "    # Market cap weight\n",
    "    # Check if we should rebalance or not depending on the return from previous period\n",
    "    if bh_ret_mc >= Nasdaq_avg_ret:\n",
    "        if bh_pf_mc.empty: # If we are holding cash, the return from this month is 0\n",
    "            bh_ret_mc = 0\n",
    "        elif len(df[df[\"pred\"]>0])<1 and rebal == 1: # If we should not rebalance, check if next month will go down\n",
    "            # If yes, and we are able to rebalance, go cash to avoid next month's crash\n",
    "            bh_ret_mc = 0 \n",
    "        else:\n",
    "            # If we are in a situation where we should not rebalance, save current stocks in the portfolio\n",
    "            tics = np.unique(df[\"tic\"])\n",
    "            bh_pf_mc = bh_pf_mc[bh_pf_mc[\"tic\"].isin(tics)].sort_values(\"tic\")\n",
    "            bh_pf_mc[\"true\"] = df[df[\"tic\"].isin(bh_pf_mc[\"tic\"].unique())].sort_values(\"tic\")[\"true\"].tolist()\n",
    "            bh_pf_mc[\"mkt_cap_pf\"] = (bh_pf_mc[\"true\"]+1)*bh_value_mc/len(bh_pf_mc) # Calculate weighted return of each stock in pf (market-cap-weighted)\n",
    "            bh_ret_mc = (sum(bh_pf_mc[\"mkt_cap_pf\"])/bh_value_mc)-1 # Find total return for month\n",
    "            bh_value_mc = sum(bh_pf_mc[\"mkt_cap_pf\"]) # Update total pf value\n",
    "    \n",
    "    # Check if returns are below threshold so we should rebalance (If we are in a month where we can NOT rebalance, we will never enter here)\n",
    "    elif bh_ret_mc < Nasdaq_avg_ret:\n",
    "        bh_pf_mc = df[df[\"pred\"]>0].copy() # Rebalance by buying every stock with positive prediction\n",
    "        if bh_pf_mc.empty: # If all predictions are negative, we go cash\n",
    "            bh_ret_mc = 0\n",
    "        elif len(bh_pf_mc)<1: # Same as above, but can be changed if we want more positive predictions before we buy\n",
    "                bh_ret_mc = 0\n",
    "        else:\n",
    "            bh_pf_mc[\"mkt_cap_pf\"] = (bh_pf_mc[\"true\"]+1)*bh_value_mc*bh_pf_mc[\"market cap\"]/sum(bh_pf_mc[\"market cap\"]) # Calculate weighted return of each stock in pf (market-cap-weighted)\n",
    "            bh_ret_mc = (sum(bh_pf_mc[\"mkt_cap_pf\"])/bh_value_mc)-1 # Find total return for month\n",
    "            bh_value_mc = sum(bh_pf_mc[\"mkt_cap_pf\"]) # Update total pf value\n",
    "            \n",
    "    # return returns from all portfolios, stocks in buy/hold and value of buy/hold\n",
    "    return ls_ret_eq, ls_ret_mc, bh_pf_eq, bh_pf_mc, bh_ret_eq, bh_ret_mc, bh_value_eq, bh_value_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372007ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:41.525502Z",
     "start_time": "2023-06-02T11:18:36.750692Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_new, portfolio_stocks, returns_list,save_i = optimal_hyper(optimal_df,X_test,y_test,dates_df_test_pf,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdaaa543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T20:12:36.522734Z",
     "start_time": "2023-05-28T20:12:36.508712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "testing = []\n",
    "for i in range(len(portfolio_stocks)):\n",
    "    if all(val > 0 for val in portfolio_stocks[i][\"true\"].head(15)):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe8a715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:24:34.146874Z",
     "start_time": "2023-05-30T09:24:34.119851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tic</th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>market cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>WYNN</td>\n",
       "      <td>0.374522</td>\n",
       "      <td>0.964447</td>\n",
       "      <td>9.097696e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>STX</td>\n",
       "      <td>0.291945</td>\n",
       "      <td>0.357737</td>\n",
       "      <td>9.272062e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>FLEX</td>\n",
       "      <td>0.276603</td>\n",
       "      <td>0.342561</td>\n",
       "      <td>7.861784e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>LOGI</td>\n",
       "      <td>0.250530</td>\n",
       "      <td>0.295720</td>\n",
       "      <td>4.809501e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>STLD</td>\n",
       "      <td>0.222925</td>\n",
       "      <td>0.413167</td>\n",
       "      <td>7.379971e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ADSK</td>\n",
       "      <td>0.222637</td>\n",
       "      <td>0.186199</td>\n",
       "      <td>7.571682e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>BB</td>\n",
       "      <td>0.222331</td>\n",
       "      <td>0.612155</td>\n",
       "      <td>6.598046e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>ISRG</td>\n",
       "      <td>0.197137</td>\n",
       "      <td>0.507236</td>\n",
       "      <td>1.042686e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.188372</td>\n",
       "      <td>0.096405</td>\n",
       "      <td>3.062847e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>QRTEA</td>\n",
       "      <td>0.181774</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>8.329880e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>EXPE</td>\n",
       "      <td>0.179608</td>\n",
       "      <td>0.498899</td>\n",
       "      <td>4.790765e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>DISH</td>\n",
       "      <td>0.172152</td>\n",
       "      <td>0.192619</td>\n",
       "      <td>6.172780e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>BIDU</td>\n",
       "      <td>0.160763</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>7.918201e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>EA</td>\n",
       "      <td>0.159710</td>\n",
       "      <td>0.118747</td>\n",
       "      <td>1.415398e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>FSLR</td>\n",
       "      <td>0.158890</td>\n",
       "      <td>0.411379</td>\n",
       "      <td>2.175794e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>MRVL</td>\n",
       "      <td>0.154826</td>\n",
       "      <td>0.198690</td>\n",
       "      <td>1.066879e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>0.154318</td>\n",
       "      <td>0.278635</td>\n",
       "      <td>2.094851e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>KLAC</td>\n",
       "      <td>0.153686</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>6.923183e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>AKAM</td>\n",
       "      <td>0.149079</td>\n",
       "      <td>0.135052</td>\n",
       "      <td>5.853070e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>EBAY</td>\n",
       "      <td>0.144258</td>\n",
       "      <td>0.311306</td>\n",
       "      <td>3.597494e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>HOLX</td>\n",
       "      <td>0.138357</td>\n",
       "      <td>0.135218</td>\n",
       "      <td>5.577574e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>TIGO</td>\n",
       "      <td>0.137226</td>\n",
       "      <td>0.308315</td>\n",
       "      <td>1.118328e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>LRCX</td>\n",
       "      <td>0.134469</td>\n",
       "      <td>0.224418</td>\n",
       "      <td>4.518135e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>CMCSA</td>\n",
       "      <td>0.125518</td>\n",
       "      <td>0.133431</td>\n",
       "      <td>3.862510e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>GEN</td>\n",
       "      <td>0.124996</td>\n",
       "      <td>0.154618</td>\n",
       "      <td>1.625636e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>SBUX</td>\n",
       "      <td>0.122562</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>1.145872e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>NTAP</td>\n",
       "      <td>0.119855</td>\n",
       "      <td>0.233154</td>\n",
       "      <td>7.145742e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>URBN</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>0.190593</td>\n",
       "      <td>5.211943e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>MCHP</td>\n",
       "      <td>0.116791</td>\n",
       "      <td>0.085418</td>\n",
       "      <td>5.648434e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>AMAT</td>\n",
       "      <td>0.115832</td>\n",
       "      <td>0.135814</td>\n",
       "      <td>2.587545e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>GRMN</td>\n",
       "      <td>0.115181</td>\n",
       "      <td>0.187647</td>\n",
       "      <td>9.239046e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>PDCO</td>\n",
       "      <td>0.113347</td>\n",
       "      <td>0.084836</td>\n",
       "      <td>3.566036e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>JNPR</td>\n",
       "      <td>0.110642</td>\n",
       "      <td>0.438538</td>\n",
       "      <td>1.164339e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>VRTX</td>\n",
       "      <td>0.103910</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>4.704844e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>XRAY</td>\n",
       "      <td>0.102551</td>\n",
       "      <td>0.065922</td>\n",
       "      <td>5.477238e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>PCAR</td>\n",
       "      <td>0.101661</td>\n",
       "      <td>0.375776</td>\n",
       "      <td>1.527427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>JBHT</td>\n",
       "      <td>0.101059</td>\n",
       "      <td>0.166321</td>\n",
       "      <td>4.150815e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>HSIC</td>\n",
       "      <td>0.093184</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>4.654914e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>FISV</td>\n",
       "      <td>0.092208</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>7.437731e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ADP</td>\n",
       "      <td>0.090261</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>2.172171e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>INTC</td>\n",
       "      <td>0.087320</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>1.230374e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>INFY</td>\n",
       "      <td>0.078836</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>2.485895e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>VRSN</td>\n",
       "      <td>0.075326</td>\n",
       "      <td>0.090620</td>\n",
       "      <td>7.359887e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>ROST</td>\n",
       "      <td>0.073678</td>\n",
       "      <td>0.057414</td>\n",
       "      <td>4.717589e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>PAYX</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.052201</td>\n",
       "      <td>1.127644e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>FAST</td>\n",
       "      <td>0.067332</td>\n",
       "      <td>0.192972</td>\n",
       "      <td>6.436062e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>0.066684</td>\n",
       "      <td>0.137649</td>\n",
       "      <td>1.255938e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>0.197013</td>\n",
       "      <td>1.476190e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>CSCO</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.152057</td>\n",
       "      <td>1.374000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>0.060735</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>1.038462e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>CTAS</td>\n",
       "      <td>0.060582</td>\n",
       "      <td>0.038026</td>\n",
       "      <td>4.074163e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>INTU</td>\n",
       "      <td>0.060008</td>\n",
       "      <td>-0.143333</td>\n",
       "      <td>8.850191e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>CHRW</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.165534</td>\n",
       "      <td>9.352633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>COST</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>0.049223</td>\n",
       "      <td>3.049512e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>CTSH</td>\n",
       "      <td>0.054881</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>9.391586e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>CHKP</td>\n",
       "      <td>0.053987</td>\n",
       "      <td>0.043224</td>\n",
       "      <td>5.173150e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>EXPD</td>\n",
       "      <td>0.044153</td>\n",
       "      <td>0.226935</td>\n",
       "      <td>9.171642e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>QCOM</td>\n",
       "      <td>0.043466</td>\n",
       "      <td>0.087638</td>\n",
       "      <td>7.182128e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>ILMN</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>4.974765e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>0.041682</td>\n",
       "      <td>0.102885</td>\n",
       "      <td>2.562087e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tic      pred      true    market cap\n",
       "794   WYNN  0.374522  0.964447  9.097696e+09\n",
       "722    STX  0.291945  0.357737  9.272062e+09\n",
       "302   FLEX  0.276603  0.342561  7.861784e+09\n",
       "494   LOGI  0.250530  0.295720  4.809501e+09\n",
       "710   STLD  0.222925  0.413167  7.379971e+09\n",
       "38    ADSK  0.222637  0.186199  7.571682e+09\n",
       "98      BB  0.222331  0.612155  6.598046e+10\n",
       "446   ISRG  0.197137  0.507236  1.042686e+10\n",
       "86    AMZN  0.188372  0.096405  3.062847e+10\n",
       "650  QRTEA  0.181774  0.827586  8.329880e+09\n",
       "266   EXPE  0.179608  0.498899  4.790765e+09\n",
       "218   DISH  0.172152  0.192619  6.172780e+09\n",
       "110   BIDU  0.160763  0.318800  7.918201e+09\n",
       "230     EA  0.159710  0.118747  1.415398e+10\n",
       "314   FSLR  0.158890  0.411379  2.175794e+10\n",
       "530   MRVL  0.154826  0.198690  1.066879e+10\n",
       "14    ADBE  0.154318  0.278635  2.094851e+10\n",
       "482   KLAC  0.153686  0.387000  6.923183e+09\n",
       "50    AKAM  0.149079  0.135052  5.853070e+09\n",
       "242   EBAY  0.144258  0.311306  3.597494e+10\n",
       "374   HOLX  0.138357  0.135218  5.577574e+09\n",
       "746   TIGO  0.137226  0.308315  1.118328e+10\n",
       "506   LRCX  0.134469  0.224418  4.518135e+09\n",
       "158  CMCSA  0.125518  0.133431  3.862510e+10\n",
       "326    GEN  0.124996  0.154618  1.625636e+10\n",
       "686   SBUX  0.122562  0.301530  1.145872e+10\n",
       "554   NTAP  0.119855  0.233154  7.145742e+09\n",
       "758   URBN  0.117336  0.190593  5.211943e+09\n",
       "518   MCHP  0.116791  0.085418  5.648434e+09\n",
       "62    AMAT  0.115832  0.135814  2.587545e+10\n",
       "362   GRMN  0.115181  0.187647  9.239046e+09\n",
       "626   PDCO  0.113347  0.084836  3.566036e+09\n",
       "470   JNPR  0.110642  0.438538  1.164339e+10\n",
       "782   VRTX  0.103910  0.072746  4.704844e+09\n",
       "806   XRAY  0.102551  0.065922  5.477238e+09\n",
       "614   PCAR  0.101661  0.375776  1.527427e+10\n",
       "458   JBHT  0.101059  0.166321  4.150815e+09\n",
       "386   HSIC  0.093184  0.026257  4.654914e+09\n",
       "290   FISV  0.092208  0.023587  7.437731e+09\n",
       "26     ADP  0.090261  0.001138  2.172171e+10\n",
       "422   INTC  0.087320  0.049900  1.230374e+11\n",
       "410   INFY  0.078836  0.156966  2.485895e+10\n",
       "770   VRSN  0.075326  0.090620  7.359887e+09\n",
       "662   ROST  0.073678  0.057414  4.717589e+09\n",
       "602   PAYX  0.070153  0.052201  1.127644e+10\n",
       "278   FAST  0.067332  0.192972  6.436062e+09\n",
       "350  GOOGL  0.066684  0.137649  1.255938e+11\n",
       "2     AAPL  0.063540  0.197013  1.476190e+11\n",
       "182   CSCO  0.062260  0.152057  1.374000e+11\n",
       "566   NVDA  0.060735  0.164300  1.038462e+10\n",
       "194   CTAS  0.060582  0.038026  4.074163e+09\n",
       "434   INTU  0.060008 -0.143333  8.850191e+09\n",
       "146   CHRW  0.056651  0.165534  9.352633e+09\n",
       "170   COST  0.056537  0.049223  3.049512e+10\n",
       "206   CTSH  0.054881  0.192400  9.391586e+09\n",
       "134   CHKP  0.053987  0.043224  5.173150e+09\n",
       "254   EXPD  0.044153  0.226935  9.171642e+09\n",
       "638   QCOM  0.043466  0.087638  7.182128e+10\n",
       "398   ILMN  0.043076  0.002954  4.974765e+09\n",
       "542   MSFT  0.041682  0.102885  2.562087e+11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_stocks[50][0:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b92af5",
   "metadata": {},
   "source": [
    "# Table for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1114a4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T11:18:48.421410Z",
     "start_time": "2023-06-02T11:18:48.368331Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nasdaq 100</th>\n",
       "      <th>Buy/hold eq</th>\n",
       "      <th>Buy/hold mc</th>\n",
       "      <th>Long/short eq</th>\n",
       "      <th>Long/short mc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total return in period</th>\n",
       "      <td>1.030968</td>\n",
       "      <td>1.398925</td>\n",
       "      <td>1.307196</td>\n",
       "      <td>2.050198</td>\n",
       "      <td>2.692237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average annual return</th>\n",
       "      <td>0.147281</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>0.186742</td>\n",
       "      <td>0.292885</td>\n",
       "      <td>0.384605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual volatility</th>\n",
       "      <td>0.160248</td>\n",
       "      <td>0.102875</td>\n",
       "      <td>0.111541</td>\n",
       "      <td>0.198170</td>\n",
       "      <td>0.311077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <td>0.778413</td>\n",
       "      <td>1.723497</td>\n",
       "      <td>1.472113</td>\n",
       "      <td>1.364202</td>\n",
       "      <td>1.163903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Nasdaq 100  Buy/hold eq  Buy/hold mc  Long/short eq  \\\n",
       "Total return in period    1.030968     1.398925     1.307196       2.050198   \n",
       "Average annual return     0.147281     0.199846     0.186742       0.292885   \n",
       "Annual volatility         0.160248     0.102875     0.111541       0.198170   \n",
       "Sharpe ratio              0.778413     1.723497     1.472113       1.364202   \n",
       "\n",
       "                        Long/short mc  \n",
       "Total return in period       2.692237  \n",
       "Average annual return        0.384605  \n",
       "Annual volatility            0.311077  \n",
       "Sharpe ratio                 1.163903  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create function that can make table of returns, volatility and Sharpe ratio for period\n",
    "def df_results(pred_period,optimal_new):\n",
    "    rf = pd.read_csv(\"DGS10.csv\") # Get risk-free rate\n",
    "    # Find Nasdaq and rf for the period we are looking at\n",
    "    if pred_period == 0:       \n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2005-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2012-01-01\")][\"NASDAQ100\"].tolist()\n",
    "        rf = rf[(rf[\"DATE\"]>\"2005\")&(rf[\"DATE\"]<\"2012\")][\"DGS10\"].mean()\n",
    "        \n",
    "    elif pred_period == 1:     \n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2012-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2019-01-01\")][\"NASDAQ100\"].tolist()\n",
    "        rf = rf[(rf[\"DATE\"]>\"2012\")&(rf[\"DATE\"]<\"2019\")][\"DGS10\"].mean()\n",
    "    \n",
    "    elif pred_period == 2:\n",
    "        plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2019-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2023-01-01\")][\"NASDAQ100\"].tolist()\n",
    "        rf = rf[(rf[\"DATE\"]>\"2019\")&(rf[\"DATE\"]<\"2023\")][\"DGS10\"].mean()\n",
    "    # The data has \".\" in a lot of places, that we remove \n",
    "    for x in plot_nas[:]:\n",
    "        if x == '.':\n",
    "            plot_nas.remove(x) \n",
    "    plot_nas = [float(x) for x in plot_nas] # The dataformat is also strings, so we convert to float\n",
    "    # Calculate Nasdaq returns\n",
    "    ret_nas = []\n",
    "    for i in range(1,len(plot_nas)):\n",
    "        ret_nas.append(plot_nas[i]/plot_nas[i-1]-1)\n",
    "    Nasdaq_ret = np.cumsum(ret_nas)[-1] # Find total Nasdaq return in period\n",
    "    Nasdaq_avg_ret = Nasdaq_ret/(len(optimal_new)/12) # Find the average yearly Nasdaq return\n",
    "    vol = np.std(ret_nas)*np.sqrt(252) # Find Nasdaq yearly volatility\n",
    "    SR = (Nasdaq_avg_ret-rf)/vol # Calculate Nasdaq Sharpe ratio\n",
    "    data = np.array([Nasdaq_ret,Nasdaq_avg_ret,vol,SR]) # Save Nasdaq data and put in df\n",
    "    df_tabular = pd.DataFrame(data=data,columns=[\"Nasdaq 100\"],index=[\"Total return in period\",\"Average annual return\",\"Annual volatility\",\"Sharpe ratio\"])\n",
    "    \n",
    "    # Allocate storage for buy/hold total return and volatility\n",
    "    tot_ret_bh = []\n",
    "    ann_vol_bh = []\n",
    "    # Allocate storage for long/short total return and volatility\n",
    "    tot_ret_ls = []\n",
    "    ann_vol_ls = []\n",
    "    \n",
    "    # Calculate buy/hold total return, average annual return, annual volatility, and Sharpe ratio\n",
    "    for col in optimal_new.columns[2:]:\n",
    "        tot_ret_bh.append(np.cumsum(optimal_new[col]).tolist()[-1])\n",
    "        avg_ann_ret_bh = [x/(len(optimal_new)/12) for x in tot_ret_bh]\n",
    "        ann_vol_bh.append(np.std(optimal_new[col])*np.sqrt(12))\n",
    "    SR_bh = (np.array(avg_ann_ret_bh)-rf)/ann_vol_bh\n",
    "    \n",
    "    # Calculate long/short total return, average annual return, annual volatility, and Sharpe ratio\n",
    "    for col in optimal_new.columns[:2]:\n",
    "        tot_ret_ls.append(np.cumsum(optimal_new[col]).tolist()[-1])\n",
    "        avg_ann_ret_ls = [x/(len(optimal_new)/12) for x in tot_ret_ls]\n",
    "        ann_vol_ls.append(np.std(optimal_new[col])*np.sqrt(12))\n",
    "    SR_ls = (np.array(avg_ann_ret_ls)-rf)/ann_vol_ls\n",
    "    \n",
    "    \n",
    "    df_tabular[\"Buy/hold eq\"] = [tot_ret_bh[0],avg_ann_ret_bh[0],ann_vol_bh[0],SR_bh[0]]\n",
    "    df_tabular[\"Buy/hold mc\"] = [tot_ret_bh[1],avg_ann_ret_bh[1],ann_vol_bh[1],SR_bh[1]]\n",
    "    df_tabular[\"Long/short eq\"] = [tot_ret_ls[0],avg_ann_ret_ls[0],ann_vol_ls[0],SR_ls[0]]\n",
    "    df_tabular[\"Long/short mc\"] = [tot_ret_ls[1],avg_ann_ret_ls[1],ann_vol_ls[1],SR_ls[1]]\n",
    "    \n",
    "    return df_tabular\n",
    "\n",
    "df_results(1,optimal_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4c82d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T19:04:04.796394Z",
     "start_time": "2023-05-27T19:04:04.319960Z"
    }
   },
   "outputs": [],
   "source": [
    "# For table with yearly long/short returns + Nasdaq\n",
    "\n",
    "a = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2005-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2012-01-01\")]\n",
    "# a = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2012-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2019-01-01\")]\n",
    "# a = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2019-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2023-01-01\")]\n",
    "for i in range(len(a)):\n",
    "    if a[\"NASDAQ100\"].iloc[i] == \".\":\n",
    "        a[\"NASDAQ100\"].iloc[i] = a[\"NASDAQ100\"].iloc[i-1]\n",
    "    else:\n",
    "        a[\"NASDAQ100\"].iloc[i] = float(a[\"NASDAQ100\"].iloc[i])\n",
    "a[\"return\"] = a[\"NASDAQ100\"]/a[\"NASDAQ100\"].shift(1)-1\n",
    "a = a.dropna()\n",
    "\n",
    "years = np.arange(2012,2019) # Change for period\n",
    "total_sum = []\n",
    "for year in years:\n",
    "    total_sum.append(sum(a[(a[\"DATE\"]>str(year-1)+\"12-31\") & (a[\"DATE\"]<str(year+1))][\"return\"].tolist()))\n",
    "\n",
    "ls_df = pd.DataFrame(index=years)\n",
    "ls_eq_cum_ret = []\n",
    "ls_mc_cum_ret = []\n",
    "for i in range(11,len(optimal_new)+11,12):\n",
    "    ls_eq_cum_ret.append(optimal_new[\"ls_ret_eq\"].loc[i-11:i].sum())\n",
    "    ls_mc_cum_ret.append(optimal_new[\"ls_ret_mc\"].loc[i-11:i].sum())\n",
    "ls_df[\"Nasdaq 100\"] = total_sum\n",
    "ls_df[\"Long/short eq\"] = ls_eq_cum_ret\n",
    "ls_df[\"Long/short mc\"] = ls_mc_cum_ret\n",
    "ls_df.loc[\"Total\"] = [sum(total_sum),sum(ls_eq_cum_ret),sum(ls_mc_cum_ret)]\n",
    "ls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68760a7",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed749c",
   "metadata": {},
   "source": [
    "In the following function, we plot the cumulative returns for nasdaq and for buy/hold with 3 different rebalancing frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5704f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-28T19:20:29.405462Z",
     "start_time": "2023-05-28T19:20:19.174982Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_return_with_nasdaq(optimal_df,Nasdaq100_index,pred_period,pf_type):\n",
    "    rebalance = 1 # Determine rebalancing frequency\n",
    "    for k in range(3):\n",
    "        if k==1:\n",
    "            rebalance = 3\n",
    "        elif k==2:\n",
    "            rebalance = 6\n",
    "        optimal_new, portfolio_stocks, returns_list, save_i = optimal_hyper(optimal_df,X_test,y_test,dates_df_test_pf,rebalance)\n",
    "        \n",
    "        # Determine period we are looking at, and find correspodning Nasdaq dates\n",
    "        if pred_period == 0:\n",
    "            date_range = pd.date_range(start='2005-01-01', end='2012-01-31', freq='M')\n",
    "            plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2005-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2012-02-01\")][\"NASDAQ100\"].tolist()\n",
    "            X_axis = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2005-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2012-01-31\")][\"DATE\"].tolist()\n",
    "        elif pred_period == 1:\n",
    "            date_range = pd.date_range(start='2012-01-01', end='2019-01-31', freq='M')\n",
    "            plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2012-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2019-02-01\")][\"NASDAQ100\"].tolist()\n",
    "            X_axis = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2012-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2019-01-31\")][\"DATE\"].tolist()\n",
    "        elif pred_period == 2:\n",
    "            date_range = pd.date_range(start='2019-01-01', end='2023-02-01', freq='M')\n",
    "            plot_nas = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2019-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2023-02-01\")][\"NASDAQ100\"].tolist()\n",
    "            X_axis = Nasdaq100_index[(Nasdaq100_index[\"DATE\"]>=\"2019-01-31\") & (Nasdaq100_index[\"DATE\"]<\"2023-01-31\")][\"DATE\"].tolist()\n",
    "        else:\n",
    "            print(\"WRONG INPUT PRED PERIOD\")\n",
    "\n",
    "        ''' PREPARE NASDAQ 100 PLOT '''\n",
    "        index_list = []\n",
    "        i = 0\n",
    "        for x in plot_nas[:]:\n",
    "            if x == '.':\n",
    "                plot_nas.remove(x) \n",
    "                index_list.append(i)\n",
    "            i = i+1\n",
    "        plot_nas = [float(x) for x in plot_nas]\n",
    "        cum_ret_nas = []\n",
    "        for i in range(1,len(plot_nas)):\n",
    "            cum_ret_nas.append(plot_nas[i]/plot_nas[i-1]-1)\n",
    "        cum_ret_nas = np.cumsum(cum_ret_nas) # Calculate cumulative returns for Nasdaq\n",
    "        \n",
    "        # Prepare x-axis for Nasdaq plot \n",
    "        X_axis = [x for i, x in enumerate(X_axis) if i not in index_list]\n",
    "        X_axis = [datetime.datetime.strptime(date_str, \"%Y-%m-%d\").date() for date_str in X_axis]\n",
    "\n",
    "        eq = [0]\n",
    "        mc = [0]\n",
    "\n",
    "        ''' PREPARE RETURN PLOT'''\n",
    "        if pf_type == \"ls\":\n",
    "            eq = eq + (optimal_new[\"ls_ret_eq\"]).tolist() # Collect lists\n",
    "            mc = mc + (optimal_new[\"ls_ret_mc\"]).tolist()\n",
    "        elif pf_type == \"bh\":\n",
    "            eq = eq + (optimal_new[\"bh_ret_eq\"]).tolist() # Collect lists\n",
    "            mc = mc + (optimal_new[\"bh_ret_mc\"]).tolist()\n",
    "        else:\n",
    "            print(\"WRONG INPUT PF TYPE\")\n",
    "\n",
    "        plot_eq = np.cumsum(eq) # Calculate cumulative returns for equal buy/hold\n",
    "        plot_mc = np.cumsum(mc) # Calculate cumulative returns for mkt cap buy/hold\n",
    "        \n",
    "        # Add recession bars to plot\n",
    "        if pred_period == 0:\n",
    "            rec_3 = [13847,14395] #rec_3: 2007-12-01, 2009-06-01\n",
    "            plt.fill_between(rec_3, -1.1, 5.1, facecolor=(0,0,0,.05), edgecolor=(0,0,0,.2))\n",
    "            plt.ylim(-0.35,2.15)\n",
    "        elif pred_period == 2:\n",
    "            rec_4 = [18292,18352] #rec_4: 2020-02-01, 2020-04-01\n",
    "            plt.fill_between(rec_4, -0.25, 1.51, facecolor=(0,0,0,.05), edgecolor=(0,0,0,.2))\n",
    "            plt.ylim(-0.2,1.35)\n",
    "        x_pf = np.array(date_range)\n",
    "        plt.style.use('default')\n",
    "        \n",
    "        # Make sure we do not plot rebalnce points if we can rebalance often or are looking at long/short\n",
    "        if len(save_i)>40 or pf_type == \"ls\":\n",
    "            marker_on = []\n",
    "        else:\n",
    "            marker_on = save_i\n",
    "        \n",
    "        # Plot\n",
    "        if k==0:\n",
    "            plt.plot(X_axis,cum_ret_nas,label = \"Nasdaq 100 index\")\n",
    "            plt.plot(x_pf,plot_eq,label = \"Equal reb 1\",color = \"dimgrey\")\n",
    "            plt.plot(x_pf,plot_mc,\"--\",label = \"Market cap reb 1\",color = \"black\")\n",
    "        \n",
    "        elif k==1:\n",
    "            plt.plot(x_pf,plot_eq,\"-o\",markevery=marker_on,label = \"Equal reb 3\",color = \"forestgreen\")\n",
    "            plt.plot(x_pf,plot_mc,\"--*\",markevery=marker_on,label = \"Market cap reb 3\",color = \"limegreen\")\n",
    "        elif k==2:\n",
    "            plt.plot(x_pf,plot_eq,\"-o\",markevery=marker_on,label = \"Equal reb 6\", color = \"tomato\")\n",
    "            plt.plot(x_pf,plot_mc,\"--*\",markevery=marker_on,label = \"Market cap reb 6\",color = \"firebrick\" )\n",
    "    \n",
    "        plt.xlabel(\"Date\",fontsize = 20)\n",
    "        plt.ylabel(\"Cumulative returns\",fontsize = 20)\n",
    "\n",
    "        plt.grid()\n",
    "        plt.legend(loc = 2, fontsize=15)\n",
    "        \n",
    "plt.figure(figsize = (15,10))\n",
    "compare_return_with_nasdaq(optimal_df,Nasdaq100_index,2,\"bh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c099a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T13:24:49.467617Z",
     "start_time": "2023-05-24T13:24:49.178810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare loss plots\n",
    "true_train = []\n",
    "true_val = []\n",
    "for j in range(100):\n",
    "    \n",
    "    temp_1 = []\n",
    "    temp_2 = []\n",
    "    for i in range(0,len(optimal_df_2019),3):\n",
    "        try:\n",
    "            temp_1.append(optimal_df_2019[\"loss_hist\"][i][\"loss\"][j])\n",
    "            temp_2.append(optimal_df_2019[\"loss_hist\"][i][\"val_loss\"][j])\n",
    "        except:\n",
    "            None\n",
    "    true_train.append(np.mean(temp_1))\n",
    "    true_val.append(np.mean(temp_2))\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(true_train[1:],label = \"training loss\")\n",
    "plt.plot(true_val[1:],label = \"validation loss\")\n",
    "plt.ylim(0.035,0.062)\n",
    "x_lab = [1,10,20,30,40,50]\n",
    "x_ticks = [0,10,20,30,40,48]\n",
    "plt.xticks(ticks=x_ticks, labels=x_lab)\n",
    "\n",
    "plt.xlabel(\"Epochs\",fontsize = 15)\n",
    "plt.ylabel(\"Loss\",fontsize = 15)\n",
    "plt.grid()\n",
    "plt.legend(loc=1,fontsize = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
