{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ddcdd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.027653Z",
     "start_time": "2023-05-04T07:39:26.013640Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sn\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8ae83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.042667Z",
     "start_time": "2023-05-04T07:39:26.029655Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define function for inserting data\n",
    "def insert_data(data,data2,unique_vals,result_list):\n",
    "    for i in tqdm_notebook(range(len(unique_vals))):\n",
    "        temp = data.loc[data['tic'] == unique_vals[i]] ; temp = temp[~temp.index.duplicated(keep='last')]\n",
    "        temp2 = data2.loc[data2['tic'] == unique_vals[i]] ; temp2 = temp2[~temp2.index.duplicated(keep='last')]\n",
    "        if not temp.empty and not temp2.empty:\n",
    "            result_list[i] = pd.concat([result_list[i], temp])\n",
    "            result_list[i] = pd.concat([result_list[i], temp2],axis=1)\n",
    "            result_list[i] = result_list[i].reset_index()\n",
    "            result_list[i] = result_list[i].fillna(method = \"ffill\")\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cd511e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.058680Z",
     "start_time": "2023-05-04T07:39:26.045673Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define dictionary for results --> Convert into list and assign some data to the dataframes\n",
    "def dic_func(unique_vals):\n",
    "    c = {}\n",
    "    for i in range(len(unique_vals)):\n",
    "        c[\"corp_{0}\".format(i)] = []\n",
    "    result_list = list(c.items())\n",
    "\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i] = pd.DataFrame()                                                     \n",
    "\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i] = pd.DataFrame(result_list[i])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5908e7a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.074695Z",
     "start_time": "2023-05-04T07:39:26.060683Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_vals_func(df):\n",
    "    unique_vals = np.unique(df['tic'])\n",
    "    return unique_vals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ec1e6",
   "metadata": {},
   "source": [
    "# Dividend-to-price ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c881f32d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.122739Z",
     "start_time": "2023-05-04T07:39:26.108726Z"
    }
   },
   "outputs": [],
   "source": [
    "def DTOP_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"DTOP\"] = np.NaN\n",
    "        if len(result_list[i])>1:\n",
    "            result_list[i][\"DTOP\"] = result_list[i][\"dvpsxm\"].shift(1)/result_list[i][\"prccm\"]\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74df66",
   "metadata": {},
   "source": [
    "# Earnings Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec69dc",
   "metadata": {},
   "source": [
    "### Accruals - balance sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d8e6a95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.137754Z",
     "start_time": "2023-05-04T07:39:26.123741Z"
    }
   },
   "outputs": [],
   "source": [
    "def ABS_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"ABS\"] = np.NaN\n",
    "        if len(result_list[i])>1:\n",
    "            result_list[i][\"ABS\"] = ((((result_list[i][\"actq\"]-result_list[i][\"actq\"].shift(1))-(result_list[i][\"chq\"]-result_list[i][\"chq\"].shift(1)))) - ((result_list[i][\"lctq\"]-result_list[i][\"lctq\"].shift(1))-(result_list[i][\"dlcq\"]-result_list[i][\"dlcq\"].shift(1)))-result_list[i][\"dpq\"])/result_list[i][\"atq\"]\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c282a",
   "metadata": {},
   "source": [
    "### Variability in sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "982b49b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.152766Z",
     "start_time": "2023-05-04T07:39:26.141757Z"
    }
   },
   "outputs": [],
   "source": [
    "def VSAL_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"VSAL\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            for j in reversed(range(len(result_list[i])-19)):\n",
    "                result_list[i].at[j+19,\"VSAL\"] = np.std(result_list[i][\"saleq\"].iloc[j:j+20])\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0876138",
   "metadata": {},
   "source": [
    "# Leverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da18e3a",
   "metadata": {},
   "source": [
    "### DTOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6c5be70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.200322Z",
     "start_time": "2023-05-04T07:39:26.187322Z"
    }
   },
   "outputs": [],
   "source": [
    "def DTOA_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"DTOA\"] = (result_list[i][\"ltq\"]+result_list[i][\"dlttq\"])/result_list[i][\"atq\"]\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38592360",
   "metadata": {},
   "source": [
    "# Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b85c0",
   "metadata": {},
   "source": [
    "### Egro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc4332d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.216336Z",
     "start_time": "2023-05-04T07:39:26.202323Z"
    }
   },
   "outputs": [],
   "source": [
    "def EGRO_func(unique_vals,result_list):\n",
    "    for i in range(len(result_list)):\n",
    "        result_list[i][\"EGRO\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "            #Make 5 years timeline for regression\n",
    "            x = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]).reshape((-1, 1))\n",
    "            temp_reg = np.zeros((len(result_list[i])))\n",
    "            temp_sum = np.zeros((len(result_list[i])))\n",
    "            k=0\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                try:\n",
    "                    temp_sum[j] = (result_list[i][\"epspxq\"][k:k+20].sum())/20\n",
    "                    if temp_sum[j]>0:\n",
    "                        temp = result_list[i][\"epspxq\"][k:k+20].to_numpy().reshape((-1, 1))\n",
    "                        model = LinearRegression().fit(x, temp)\n",
    "                        temp_reg[j] = model.coef_*(-1)\n",
    "                    else:\n",
    "                        temp_sum[j] = np.NaN\n",
    "                        temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "                except:\n",
    "                    temp_sum[j] = np.NaN\n",
    "                    temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "       \n",
    "            result_list[i][\"EGRO\"] = temp_reg/temp_sum\n",
    "            result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d47f2",
   "metadata": {},
   "source": [
    "### SGRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340b72a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.248366Z",
     "start_time": "2023-05-04T07:39:26.218338Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGRO_func(unique_vals,result_list):\n",
    "    for i in range(len(result_list)):\n",
    "        result_list[i][\"sales per share\"] = result_list[i][\"saleq\"] / result_list[i][\"cshoq\"]\n",
    "        result_list[i][\"SGRO\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "            #Make 5 years timeline for regression\n",
    "            x = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]).reshape((-1, 1))\n",
    "            temp_reg = np.zeros((len(result_list[i])))\n",
    "            temp_sum = np.zeros((len(result_list[i])))\n",
    "            k=0\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                try:\n",
    "                    temp = result_list[i][\"sales per share\"][k:k+20].to_numpy().reshape((-1, 1))\n",
    "                    model = LinearRegression().fit(x, temp)\n",
    "                    temp_reg[j] = model.coef_*(-1)                \n",
    "                    temp_sum[j] = (result_list[i][\"sales per share\"][k:k+20].sum())/20\n",
    "                    k = k+1\n",
    "\n",
    "                except:\n",
    "                    temp_sum[j] = np.NaN\n",
    "                    temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "\n",
    "            result_list[i][\"SGRO\"] = temp_reg/temp_sum\n",
    "            result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a6c4d",
   "metadata": {},
   "source": [
    "# Liquidity (Both monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe04dc8",
   "metadata": {},
   "source": [
    "### Montly share turnover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e2e3f",
   "metadata": {},
   "source": [
    "### Quarterly turnover "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d95d986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.264380Z",
     "start_time": "2023-05-04T07:39:26.250368Z"
    }
   },
   "outputs": [],
   "source": [
    "def STOM_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"STOM\"] = np.log(result_list[i][\"cshtrm\"]/result_list[i][\"cshom\"])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fec33733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.279907Z",
     "start_time": "2023-05-04T07:39:26.265381Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def STOQ_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"STOQ\"] = np.NaN\n",
    "        if len(result_list[i])>2:\n",
    "            for j in reversed(range(len(result_list[i])-2)):\n",
    "                x = np.exp(result_list[i][\"STOM\"].iloc[j+2])+np.exp(result_list[i][\"STOM\"].iloc[j+1])+np.exp(result_list[i][\"STOM\"].iloc[j])\n",
    "                result_list[i].at[j,\"STOQ\"] = np.log(1/3*x)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b3c39",
   "metadata": {},
   "source": [
    "# Management quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93d040",
   "metadata": {},
   "source": [
    "### Asset growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75ae7f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.295921Z",
     "start_time": "2023-05-04T07:39:26.280908Z"
    }
   },
   "outputs": [],
   "source": [
    "def AGRO_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"AGRO\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "            #Make 5 years timeline for regression\n",
    "            x = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]).reshape((-1, 1))\n",
    "            temp_reg = np.zeros((len(result_list[i])))\n",
    "            temp_sum = np.zeros((len(result_list[i])))\n",
    "            k=0\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                try:\n",
    "                    temp = result_list[i][\"atq\"][k:k+20].to_numpy().reshape((-1, 1))\n",
    "                    model = LinearRegression().fit(x, temp)\n",
    "                    temp_reg[j] = model.coef_*(-1)                \n",
    "                    temp_sum[j] = (result_list[i][\"atq\"][k:k+20].sum())/20\n",
    "                    k = k+1\n",
    "\n",
    "                except:\n",
    "                    temp_sum[j] = np.NaN\n",
    "                    temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "\n",
    "            result_list[i][\"AGRO\"] = temp_reg/temp_sum\n",
    "            result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbff592",
   "metadata": {},
   "source": [
    "### Insuance growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b1e4f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.311936Z",
     "start_time": "2023-05-04T07:39:26.297924Z"
    }
   },
   "outputs": [],
   "source": [
    "def IGRO_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"IGRO\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "            #Make 5 years timeline for regression\n",
    "            x = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]).reshape((-1, 1))\n",
    "            temp_reg = np.zeros((len(result_list[i])))\n",
    "            temp_sum = np.zeros((len(result_list[i])))\n",
    "            k=0\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                try:\n",
    "                    temp = result_list[i][\"cshoq\"][k:k+20].to_numpy().reshape((-1, 1))\n",
    "                    model = LinearRegression().fit(x, temp)\n",
    "                    temp_reg[j] = model.coef_*(-1)                \n",
    "                    temp_sum[j] = (result_list[i][\"cshoq\"][k:k+20].sum())/20\n",
    "                    k = k+1\n",
    "\n",
    "                except:\n",
    "                    temp_sum[j] = np.NaN\n",
    "                    temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "\n",
    "            result_list[i][\"IGRO\"] = temp_reg/temp_sum\n",
    "            result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7709c",
   "metadata": {},
   "source": [
    "### Capital expenditure growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a66b1ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.327952Z",
     "start_time": "2023-05-04T07:39:26.312937Z"
    }
   },
   "outputs": [],
   "source": [
    "def CXGRO_func(unique_vals,result_list): \n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"CXGRO\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "            #Make 5 years timeline for regression\n",
    "            x = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]).reshape((-1, 1))\n",
    "            temp_reg = np.zeros((len(result_list[i])))\n",
    "            temp_sum = np.zeros((len(result_list[i])))\n",
    "            k=0\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                try:\n",
    "                    temp = result_list[i][\"capxy\"][k:k+20].to_numpy().reshape((-1, 1))\n",
    "                    model = LinearRegression().fit(x, temp)\n",
    "                    temp_reg[j] = model.coef_*(-1)                \n",
    "                    temp_sum[j] = (result_list[i][\"capxy\"][k:k+20].sum())/20\n",
    "                    k = k+1\n",
    "\n",
    "                except:\n",
    "                    temp_sum[j] = np.NaN\n",
    "                    temp_reg[j] = np.NaN\n",
    "                    k = k+1\n",
    "\n",
    "            result_list[i][\"CXGRO\"] = temp_reg/temp_sum\n",
    "            result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274efd2",
   "metadata": {},
   "source": [
    "### Capital expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "778be662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.343965Z",
     "start_time": "2023-05-04T07:39:26.330954Z"
    }
   },
   "outputs": [],
   "source": [
    "def CX_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i] = result_list[i].sort_index(ascending=False)\n",
    "        result_list[i][\"CX\"] = np.NaN\n",
    "        if len(result_list[i])>19:\n",
    "            for j in range(len(result_list[i])-19):\n",
    "                result_list[i].at[j,\"CX\"] = result_list[i][\"capxy\"].iloc[j]/((result_list[i][\"capxy\"][j:j+20].sum())/20)\n",
    "        \n",
    "        result_list[i] = result_list[i].sort_index(ascending=True)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0187e5",
   "metadata": {},
   "source": [
    "# Profitability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cb4a7",
   "metadata": {},
   "source": [
    "### Asset turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee1d8bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.359980Z",
     "start_time": "2023-05-04T07:39:26.345967Z"
    }
   },
   "outputs": [],
   "source": [
    "def ATO_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"ATO\"] = result_list[i][\"saleq\"] / result_list[i][\"atq\"]\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4c8e9",
   "metadata": {},
   "source": [
    "### Gross profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ded8e9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.375996Z",
     "start_time": "2023-05-04T07:39:26.361983Z"
    }
   },
   "outputs": [],
   "source": [
    "def GP_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"GP\"] = (result_list[i][\"saleq\"]-result_list[i][\"cogsq\"]) / result_list[i][\"atq\"]\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bfa47",
   "metadata": {},
   "source": [
    "### Gross margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c306a0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.392009Z",
     "start_time": "2023-05-04T07:39:26.377997Z"
    }
   },
   "outputs": [],
   "source": [
    "def GM_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"GM\"] = (result_list[i][\"saleq\"]-result_list[i][\"cogsq\"]) / result_list[i][\"saleq\"]\n",
    "    return result_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a309e",
   "metadata": {},
   "source": [
    "# Prospect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db0a79",
   "metadata": {},
   "source": [
    "### Maximum drawdown (12- Months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9ebb335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.440052Z",
     "start_time": "2023-05-04T07:39:26.426040Z"
    }
   },
   "outputs": [],
   "source": [
    "def MAD_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"MAD\"] = np.NaN\n",
    "        if len(result_list[i])>11:\n",
    "            for j in reversed(range(len(result_list[i])-11)):\n",
    "                result_list[i].at[j+11,\"MAD\"] = result_list[i][\"prccm\"][j:j+12].max()-result_list[i][\"prccm\"][j:j+12].min()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352ab11",
   "metadata": {},
   "source": [
    "# Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d47c06ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.456067Z",
     "start_time": "2023-05-04T07:39:26.447060Z"
    }
   },
   "outputs": [],
   "source": [
    "def SEASON_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"Total average\"] = np.NaN\n",
    "        result_list[i][\"Seasonal average\"] = np.NaN\n",
    "        result_list[i][\"SEASON\"] = np.NaN\n",
    "        if len(result_list[i])>59:\n",
    "            for j in reversed(range(len(result_list[i])-60)):\n",
    "                result_list[i].at[j+60,\"Total average\"] = result_list[i][\"trt1m\"][j+1:j+61].sum()/60\n",
    "                result_list[i].at[j+60,\"Seasonal average\"] = result_list[i][\"trt1m\"].iloc[j+12:j+60+12:12].sum()/5\n",
    "\n",
    "            result_list[i][\"SEASON\"] = result_list[i][\"Seasonal average\"]/result_list[i][\"Total average\"]\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4eb553",
   "metadata": {},
   "source": [
    "# Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bace2b",
   "metadata": {},
   "source": [
    "### Log of market cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e222c65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.472081Z",
     "start_time": "2023-05-04T07:39:26.458069Z"
    }
   },
   "outputs": [],
   "source": [
    "def LNCAP_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"LNCAP\"] =  np.log(result_list[i][\"prccm\"] * result_list[i][\"cshom\"])\n",
    "    return result_list   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be54d4",
   "metadata": {},
   "source": [
    "# Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e945a",
   "metadata": {},
   "source": [
    "### Book-to-price ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8595a5e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.488098Z",
     "start_time": "2023-05-04T07:39:26.474084Z"
    }
   },
   "outputs": [],
   "source": [
    "def BTOP_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"BTOP\"] = (result_list[i][\"atq\"]-result_list[i][\"ltq\"])/(result_list[i][\"cshoq\"]*result_list[i][\"prccq\"])\n",
    "    return result_list     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fb76a",
   "metadata": {},
   "source": [
    "### Sales-to-price ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a79096c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:26.504111Z",
     "start_time": "2023-05-04T07:39:26.491100Z"
    }
   },
   "outputs": [],
   "source": [
    "def STOP_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"STOP\"] =  result_list[i][\"saleq\"]/(result_list[i][\"cshoq\"]*result_list[i][\"prccq\"])\n",
    "    return result_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2740e",
   "metadata": {},
   "source": [
    "# Volatility & momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316461e",
   "metadata": {},
   "source": [
    "### Annualized volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47b164f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:07:51.987397Z",
     "start_time": "2023-05-04T08:07:51.981392Z"
    }
   },
   "outputs": [],
   "source": [
    "def annualized_volatility_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"ANN VOL\"] = np.NaN\n",
    "        if len(result_list[i])>11:\n",
    "            for j in range(11,len(result_list[i])):\n",
    "                result_list[i].at[j,\"ANN VOL\"] = np.std(result_list[i][\"trt1m\"][j-11:j+1])*np.sqrt(12)            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0360d7",
   "metadata": {},
   "source": [
    "### 1-month & 1-year momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dc8be12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:21:15.576141Z",
     "start_time": "2023-05-04T08:21:15.559127Z"
    }
   },
   "outputs": [],
   "source": [
    "def momentum_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"1M MOM\"] = result_list[i][\"prccm\"]-result_list[i][\"prccm\"].shift(1)\n",
    "        result_list[i][\"1Y MOM\"] = result_list[i][\"prccm\"]-result_list[i][\"prccm\"].shift(12)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0da5c6",
   "metadata": {},
   "source": [
    "### Short and long term reversal, medium momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16a344bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:21:16.516518Z",
     "start_time": "2023-05-04T08:21:16.505498Z"
    }
   },
   "outputs": [],
   "source": [
    "def slm_func(unique_vals,result_list):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"SHOREV\"] = result_list[i][\"trt1m\"].shift(1)\n",
    "        result_list[i][\"MED MOM\"] = result_list[i][\"prccm\"].shift(2)/result_list[i][\"prccm\"].shift(12)-1\n",
    "        result_list[i][\"LONREV\"] = result_list[i][\"prccm\"].shift(13)/result_list[i][\"prccm\"].shift(60)-1\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92d106",
   "metadata": {},
   "source": [
    "### Downside Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "296253d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T09:35:16.735998Z",
     "start_time": "2023-05-04T09:35:16.729992Z"
    }
   },
   "outputs": [],
   "source": [
    "def downside_func(unique_vals,result_list):\n",
    "    exp_ret = 0.06\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"deviation\"] = result_list[i][\"trt1m\"]-exp_ret\n",
    "        result_list[i][\"DOWNSIDE\"] = np.abs(result_list[i][result_list[i][\"deviation\"]<0][\"deviation\"])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a0f08",
   "metadata": {},
   "source": [
    "# Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f807146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T09:37:13.254960Z",
     "start_time": "2023-05-04T09:37:13.197841Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function for running all descriptors\n",
    "def run_all(unique_vals,result_list):\n",
    "    result_list = DTOP_func(unique_vals,result_list)\n",
    "    result_list = ABS_func(unique_vals,result_list)\n",
    "    result_list = VSAL_func(unique_vals,result_list)\n",
    "    result_list = DTOA_func(unique_vals,result_list)\n",
    "    result_list = EGRO_func(unique_vals,result_list)\n",
    "    result_list = SGRO_func(unique_vals,result_list)\n",
    "    result_list = STOM_func(unique_vals,result_list)\n",
    "    result_list = STOQ_func(unique_vals,result_list)\n",
    "    result_list = AGRO_func(unique_vals,result_list)\n",
    "    result_list = IGRO_func(unique_vals,result_list)\n",
    "    result_list = CXGRO_func(unique_vals,result_list)\n",
    "    result_list = CX_func(unique_vals,result_list)\n",
    "    result_list = ATO_func(unique_vals,result_list)\n",
    "    result_list = GP_func(unique_vals,result_list)\n",
    "    result_list = GM_func(unique_vals,result_list)\n",
    "    result_list = MAD_func(unique_vals,result_list)\n",
    "    result_list = SEASON_func(unique_vals,result_list)\n",
    "    result_list = LNCAP_func(unique_vals,result_list)\n",
    "    result_list = BTOP_func(unique_vals,result_list)\n",
    "    result_list = STOP_func(unique_vals,result_list)\n",
    "    result_list = annualized_volatility_func(unique_vals,result_list)\n",
    "    result_list = momentum_func(unique_vals,result_list)\n",
    "    result_list = slm_func(unique_vals,result_list)\n",
    "    result_list = downside_func(unique_vals,result_list)\n",
    "    result_list = insert_tic_and_fill(result_list,unique_vals)\n",
    "    return result_list\n",
    "\n",
    "#Insert ticker\n",
    "def insert_tic_and_fill(result_list,unique_vals):\n",
    "    for i in range(len(unique_vals)):\n",
    "        result_list[i][\"tic\"] = unique_vals[i]\n",
    "        result_list[i] = result_list[i].fillna(method='ffill')\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb91b5",
   "metadata": {},
   "source": [
    "# Quarterly + yearly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e87c6e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:39:52.831534Z",
     "start_time": "2023-05-04T07:39:44.582183Z"
    }
   },
   "outputs": [],
   "source": [
    "quarterly_all = pd.read_csv(\"QUATERLY ALL RAW.csv\"); annual_all = pd.read_csv(\"ANNUAL ALL RAW.csv\")\n",
    "\n",
    "Fed_funds_raw = pd.read_csv(\"FEDFUNDS.csv\") ; Fed_funds_raw[\"datadate\"] = pd.to_datetime(Fed_funds_raw[\"DATE\"]) ; Fed_funds_raw = Fed_funds_raw.set_index(\"datadate\")\n",
    "GDP_raw = pd.read_csv(\"GDP.csv\") ; GDP_raw[\"datadate\"] = pd.to_datetime(GDP_raw[\"DATE\"]) ; GDP_raw = GDP_raw.set_index(\"datadate\")\n",
    "TB3MS_raw = pd.read_csv(\"TB3MS.csv\") ; TB3MS_raw[\"datadate\"] = pd.to_datetime(TB3MS_raw[\"DATE\"]) ; TB3MS_raw = TB3MS_raw.set_index(\"datadate\")\n",
    "DGS3_raw = pd.read_csv(\"DGS3.csv\") ; DGS3_raw[\"datadate\"] = pd.to_datetime(DGS3_raw[\"DATE\"]) ; DGS3_raw = DGS3_raw.set_index(\"datadate\")\n",
    "DGS10_raw = pd.read_csv(\"DGS10.csv\") ; DGS10_raw[\"datadate\"] = pd.to_datetime(DGS10_raw[\"DATE\"]) ; DGS10_raw = DGS10_raw.set_index(\"datadate\")\n",
    "UNRATE_raw = pd.read_csv(\"UNRATE.csv\"); UNRATE_raw[\"datadate\"] = pd.to_datetime(UNRATE_raw[\"DATE\"]) ; UNRATE_raw = UNRATE_raw.set_index(\"datadate\")\n",
    "\n",
    "Macro_df = pd.concat([Fed_funds_raw,GDP_raw[\"GDP_PCH\"],TB3MS_raw[\"TB3MS\"],DGS3_raw[\"DGS3\"],DGS10_raw[\"DGS10\"],UNRATE_raw[\"UNRATE\"]],axis=1)\n",
    "Macro_df = Macro_df[:-1] #Delete last row since this is empty\n",
    "Macro_df['DGS3'] = Macro_df['DGS3'].astype(float) #convert string to float\n",
    "Macro_df[\"DGS_diff\"] = Macro_df[\"DGS3\"]-Macro_df[\"DGS10\"]\n",
    "Macro_df[\"rf\"] = Macro_df[\"DGS10\"]\n",
    "Macro_df = Macro_df.drop([\"DGS3\",\"DGS10\"],axis=1)\n",
    "Macro_df = Macro_df.shift(-1)\n",
    "\n",
    "\n",
    "quarterly_filtered = quarterly_all.loc[((quarterly_all['exchg'] == 11.0) | (quarterly_all['exchg'] == 12.0) | (quarterly_all['exchg'] == 14.0) )]\n",
    "annual_filtered = annual_all.loc[((annual_all['exchg'] == 11.0) | (annual_all['exchg'] == 12.0) | (annual_all['exchg'] == 14.0) )]\n",
    "\n",
    "quarterly_filtered = quarterly_filtered[~pd.isnull(quarterly_filtered[\"tic\"])]; annual_filtered = annual_filtered[~pd.isnull(annual_filtered[\"tic\"])]\n",
    "quarterly_filtered = quarterly_filtered.drop_duplicates(); annual_filtered = annual_filtered.drop_duplicates()\n",
    "\n",
    "quarterly_filtered = quarterly_filtered.sort_values([\"tic\",\"datadate\"]); annual_filtered = annual_filtered.sort_values([\"tic\",\"datadate\"])\n",
    "\n",
    "quarterly_filtered[\"datadate\"] = pd.to_datetime(quarterly_filtered[\"datadate\"]) ; annual_filtered[\"datadate\"] = pd.to_datetime(annual_filtered[\"datadate\"])\n",
    "\n",
    "quarterly_filtered = quarterly_filtered.reset_index(drop = True) ; annual_filtered = annual_filtered.reset_index(drop = True)\n",
    "quarterly_filtered = quarterly_filtered.set_index(\"datadate\") ; annual_filtered = annual_filtered.set_index(\"datadate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4da16",
   "metadata": {},
   "source": [
    "# Monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31898379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:40:31.252344Z",
     "start_time": "2023-05-04T07:39:52.833536Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_all = pd.read_csv(\"MONTHLY ALL RAW.csv\")\n",
    "monthly_all[\"trt1m\"] = monthly_all[\"trt1m\"]/100\n",
    "monthly_all = monthly_all.drop_duplicates()\n",
    "monthly_all = monthly_all.reset_index(drop = True)\n",
    "monthly_all[\"datadate\"] = pd.to_datetime(monthly_all[\"datadate\"])\n",
    "monthly_all = monthly_all.set_index(\"datadate\")\n",
    "\n",
    "monthly_all_merged = pd.merge_asof(monthly_all.sort_index(), Macro_df, on=\"datadate\")\n",
    "monthly_all_merged[\"tic\"] = monthly_all_merged[\"tic\"].astype(str) \n",
    "monthly_all_merged['FEDFUNDS'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged['GDP_PCH'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged['TB3MS'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged['DGS_diff'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged['UNRATE'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged['rf'].mask(monthly_all_merged['datadate'] > '2023-02-01', np.nan, inplace=True)\n",
    "monthly_all_merged = monthly_all_merged.set_index(\"datadate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5428d9f",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd64d2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T07:59:30.294616Z",
     "start_time": "2023-05-04T07:40:31.270360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6c309b70914910bb6189c1dfb00886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_vals_mon = unique_vals_func(monthly_all_merged)\n",
    "unique_vals_quarterly = unique_vals_func(quarterly_filtered)\n",
    "unique_vals_quarterly = unique_vals_quarterly[~pd.isnull(unique_vals_quarterly)]\n",
    "unique_vals = np.intersect1d(unique_vals_mon,unique_vals_quarterly)\n",
    "monthly_all_merged = monthly_all_merged[monthly_all_merged[\"tic\"].isin(unique_vals)]\n",
    "\n",
    "result_list = dic_func(unique_vals)\n",
    "result_list = insert_data(quarterly_filtered,monthly_all_merged,unique_vals,result_list)\n",
    "\n",
    "\n",
    "# remove inconsistensies\n",
    "for i in range(len(unique_vals)):\n",
    "    result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"cshom\",\"prccm\"]] = result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"cshom\",\"prccm\"]].replace({0: np.nan})\n",
    "    result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"rf\",\"cshom\",\"prccm\"]] = result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"rf\",\"cshom\",\"prccm\"]].fillna(method='ffill')\n",
    "    result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"rf\",\"cshom\",\"prccm\"]] = result_list[i][[\"atq\",\"prccq\",\"epspxq\",\"saleq\",\"cshoq\",\"rf\",\"cshom\",\"prccm\"]].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0ec5724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T12:30:39.799970Z",
     "start_time": "2023-05-04T09:37:34.067281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/23\n",
      "1/23\n",
      "2/23\n",
      "3/23\n",
      "4/23\n",
      "5/23\n",
      "6/23\n",
      "7/23\n",
      "8/23\n",
      "9/23\n",
      "10/23\n",
      "11/23\n",
      "12/23\n",
      "13/23\n",
      "14/23\n",
      "15/23\n",
      "16/23\n",
      "17/23\n",
      "18/23\n",
      "19/23\n",
      "20/23\n",
      "21/23\n",
      "22/23\n",
      "23/23\n",
      "24/23\n",
      "25/23\n",
      "26/23\n"
     ]
    }
   ],
   "source": [
    "result_list = run_all(unique_vals,result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124725a",
   "metadata": {},
   "source": [
    "# Create final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8d0e2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T13:21:13.539915Z",
     "start_time": "2023-05-04T13:20:06.431543Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df_raw = pd.concat(result_list)\n",
    "#Make datadate the index\n",
    "#result_df_raw[\"datadate\"] = pd.to_datetime(result_df_raw[\"index\"])\n",
    "#result_df_raw = result_df_raw.drop([\"index\"],axis=1)\n",
    "result_df_raw = result_df_raw.set_index(\"datadate\")\n",
    "result_df_raw = result_df_raw.sort_index()\n",
    "\n",
    "result_df = result_df_raw[[\"tic\",\"MIDREV\",\"MIDREV excess\",\"DTOP\",\"ABS\",\"VSAL\",\"DTOA\",\"EGRO\",\"SGRO\",\"STOM\",\"STOQ\",\"AGRO\",\"IGRO\",\"CXGRO\",\"CX\",\"ATO\",\"GP\",\"GM\",\"MAD\",\"SEASON\",\"LNCAP\",\"BTOP\",\"STOP\",\"ANN VOL\",\"1M MOM\",\"1Y MOM\",\"SHOREV\",\"MED MOM\",\"LONREV\",\"DOWNSIDE\",\"return quarterly\",\"trt1m\",\"FEDFUNDS\",\"GDP_PCH\",\"TB3MS\",\"DGS_diff\",\"UNRATE\",\"rf\",\"ggroup\",\"gsector\",\"naics\",\"sic\"]]\n",
    "#result_df_GIC = result_df_raw[[\"tic\",\"ggroup\",\"gind\",\"gsector\",\"gsubind\"]]\n",
    "\n",
    "#Output final csv file\n",
    "result_df.to_csv(\"sorted_dates.csv\")\n",
    "#result_df_raw.to_csv(\"results_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef83123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T09:21:49.114679Z",
     "start_time": "2023-03-14T09:21:49.114679Z"
    }
   },
   "outputs": [],
   "source": [
    "#If one wants to load in the data in this jupytor notebook\n",
    "df = pd.read_csv(\"sorted_dates.csv\") # Load in unfinished dataset\n",
    "\n",
    "df_vix = pd.read_csv(\"VIXCLS.csv\") # Load in VIX (we decided to add it late)\n",
    "# Make manipulations to VIX df and merge it to dataset\n",
    "df_vix[\"datadate\"] = df_vix[\"DATE\"]\n",
    "df_vix[\"datadate\"] = pd.to_datetime(df_vix[\"datadate\"])\n",
    "df[\"datadate\"] = pd.to_datetime(df[\"datadate\"])\n",
    "df_vix = df_vix.drop([\"DATE\"],axis = 1)\n",
    "df = pd.merge_asof(df,df_vix, on =\"datadate\")\n",
    "\n",
    "df_close = pd.read_csv(\"Closing_price_month.csv\") # Load in closing price monthly (we noticed we might need it)\n",
    "df_close[\"datadate\"] = pd.to_datetime(df_close[\"datadate\"])\n",
    "df_close = df_close[[\"tic\",\"datadate\",\"prccm\"]]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = df.fillna(0) #Fill NaN with 0\n",
    "# Remove a bunch of unused columns from dataset\n",
    "df = df.loc[:, ~df.columns.isin([\"trt1m\",\"return quarterly\",\"tic.1\",\"ggroup.1\", 'ggroup', \"gsector\",\"gsector.1\",\"naics.1\",\"naics\",\"sic.1\",\"sic\"])]\n",
    "df = df.sort_values([\"tic\",\"datadate\"]) # Sort dataset on tickers and datadate, to get in correct order\n",
    "df = pd.merge(df, df_close, on=['tic', 'datadate']) # Merge closing price to dataset\n",
    "# Remove two firms with wildly inconsistent price and return data\n",
    "df = df[df[\"tic\"]!=\"CRGE\"]\n",
    "df = df[df[\"tic\"]!=\"HYMC\"]\n",
    "\n",
    "df[\"log_ret\"] = np.log(df[\"prccm\"]/df[\"prccm\"].shift(1)) # Include log-return from \"current\" month as feature\n",
    "df[\"target\"] = np.log(df[\"prccm\"].shift(-1)/df[\"prccm\"]) # We want to \"forecast\" (predict next month's log-return)\n",
    "\n",
    "# Load in monthly dataset since we need some information from here for the portfolio stuff\n",
    "monthly = pd.read_csv(\"MONTHLY ALL RAW.csv\")\n",
    "monthly = monthly[[\"tic\",\"datadate\",\"prccm\",\"cshom\"]] # Only interested in ticker, datadate, closing price, and shares outstanding\n",
    "monthly[\"mkt cap\"] = monthly[\"prccm\"] * monthly[\"cshom\"] # Calculate market cap (used in market cap weighted portfolios)\n",
    "monthly = monthly.drop([\"prccm\",\"cshom\"],axis=1) # After market cap is calculated, we have no need for closing price and shares outstanding\n",
    "# Make manipulations and merge to dataset\n",
    "monthly[\"datadate\"] = pd.to_datetime(monthly[\"datadate\"]) \n",
    "df = pd.merge(df, monthly, on=['tic', 'datadate'])\n",
    "df[\"mkt cap\"] = df[\"mkt cap\"].fillna(0) # We noticed a few places with NaN values in market cap, remove these\n",
    "\n",
    "\n",
    "# The next few lines correct an issue we had, where, when calculating log-returns, some firms' final datapoints would use\n",
    "# the incorrect data from another firm. This is obviously not correct, so we remove every firm's final datapoint\n",
    "df = df.reset_index(drop = True)\n",
    "temp = [0] # Create temp for storing index\n",
    "for i in tqdm_notebook(range(len(df)-1)):\n",
    "    if df[\"tic\"].iloc[i] != df[\"tic\"].iloc[i+1]: # Find where we need to delete a datapoint (where NaN should have appead)\n",
    "        temp.append(i) # Append the index\n",
    "        temp.append(i+1)\n",
    "temp.append(len(df)-1)\n",
    "df = df.drop(temp) # Remove the index from df\n",
    "df = df.dropna() # Remove NaN values\n",
    "df = df.reset_index(drop = True)\n",
    "df = df.sort_values([\"tic\",\"datadate\"]) # Sort dataset on tickers and datadate, to get in correct order, again\n",
    "\n",
    "n_features = 31 # Choose amount of features for use in later function. We have 31\n",
    "\n",
    "input_size = 12 # Determine the amount of data for each firm that should pÃ¥ put in the model. We choose 12 months\n",
    "train_size = 0.8 # NOT USED!!! - old variable from when we trained on 80% and tested on 20% of data\n",
    "\n",
    "df = df[df[\"datadate\"]<\"2023\"] # We decided for simplicity to cut off the data at 2023\n",
    "\n",
    "# The next lines are what separates the data in the aforementioned periods\n",
    "#df = df[(df[\"datadate\"]<\"2012-01-01\") & (df[\"datadate\"]>\"2004-02-01\")] \n",
    "#df = df[(df[\"datadate\"]<\"2019-01-01\") & (df[\"datadate\"]>\"2011-02-01\")] \n",
    "#df = df[(df[\"datadate\"]<\"2023-01-01\") & (df[\"datadate\"]>\"2018-02-01\")] \n",
    "\n",
    "df = df[(df[\"datadate\"]<\"2005\") & (df[\"datadate\"]>\"1989\")] \n",
    "\n",
    "# After examining descriptor correlation, these were deemed not important, and thus dropped\n",
    "df = df.drop([\"STOQ\",\"ATO\",\"TB3MS\",\"rf\"],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c8542",
   "metadata": {},
   "source": [
    "# Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sorted_dates.csv\")\n",
    "df_vix = pd.read_csv(\"VIXCLS.csv\")\n",
    "df_vix[\"datadate\"] = df_vix[\"DATE\"]\n",
    "df_vix[\"datadate\"] = pd.to_datetime(df_vix[\"datadate\"])\n",
    "df[\"datadate\"] = pd.to_datetime(df[\"datadate\"])\n",
    "df_vix = df_vix.drop([\"DATE\"],axis = 1)\n",
    "df = pd.merge_asof(df,df_vix, on =\"datadate\")\n",
    "df = df[df[\"trt1m\"].notna()]\n",
    "df = df[df[\"trt1m\"]<20]\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.fillna(0)\n",
    "df = df.loc[:, ~df.columns.isin([\"return quarterly\",\"tic.1\",\"ggroup.1\", 'ggroup', \"gsector\",\"gsector.1\",\"naics.1\",\"naics\",\"sic.1\",\"sic\"])]\n",
    "df1 = df.pop('trt1m') # remove column trt1m and store it in df1\n",
    "df['trt1m']=df1 # add trt1m series as a 'new' column.\n",
    "df = pd.read_csv(\"Final_data.csv\")\n",
    "df = df.drop([\"Unnamed: 0\",\"rf\",\"DOWNSIDE\",\"target\"],axis = 1)\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(40,40))\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "np.tril(np.ones(corr_matrix.shape)).astype(bool)\n",
    "df_lt = corr_matrix.where(np.tril(np.ones(corr_matrix.shape)).astype(bool))\n",
    "plt.figure(1,figsize=(20,20))\n",
    "ax=sn.heatmap(df_lt,annot=True,fmt='.1g',cmap=\"coolwarm\",square=True,cbar_kws={\"shrink\": 0.825})\n",
    "for text in ax.texts:\n",
    "    text.set_size(6.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d15878",
   "metadata": {},
   "source": [
    "# Unique firms plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(1990,2023,1)\n",
    "distribution_tics = []\n",
    "for year in years:\n",
    "    x = len(np.unique(df[(df[\"datadate\"]>str(year)) & (df[\"datadate\"]<str(year+1))][\"tic\"]))\n",
    "    distribution_tics.append(x)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(years,distribution_tics)\n",
    "plt.axhline(y = np.mean(distribution_tics), color = 'k', linestyle = '--',label=\"Mean no. of firms per year\")\n",
    "plt.xlim(1989,2023)\n",
    "years_3 = np.arange(1990,2023,3)\n",
    "plt.xticks(ticks=years_3, labels=years_3)\n",
    "plt.title(\"Number of unique firms each year\",fontsize=28)\n",
    "plt.grid(axis=\"y\",alpha=0.5)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.legend(loc=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9d1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"testtestset.csv\")\n",
    "data = data.loc[((data['exchg'] == 11.0) | (data['exchg'] == 12.0) | (data['exchg'] == 14.0) )]\n",
    "data = data[~pd.isnull(data[\"tic\"])]\n",
    "data[\"datadate\"] = pd.to_datetime(data[\"datadate\"])\n",
    "years = np.arange(1990,2023,1)\n",
    "distribution_tics_inac = []\n",
    "for year in years:\n",
    "    x = len(np.unique(data[(data[\"datadate\"]>str(year)) & (data[\"datadate\"]<str(year+1))][\"tic\"]))\n",
    "    distribution_tics_inac.append(x)\n",
    "    print(str(year)+\":\",x,\"unique tickers\")\n",
    "print(\"Total unique tickers:\",len(np.unique(data[\"tic\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(years,distribution_tics_inac)\n",
    "plt.axhline(y = np.mean(distribution_tics_inac), color = 'k', linestyle = '--',label=\"Mean no. of firms per year\")\n",
    "plt.xlim(1989,2024)\n",
    "years_3 = np.arange(1990,2023,3)\n",
    "plt.xticks(ticks=years_3, labels=years_3)\n",
    "plt.title(\"Number of unique firms each year\",fontsize=28)\n",
    "plt.grid(axis=\"y\",alpha=0.5)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "# Plot bar graphs of unique firm distributions\n",
    "plt.bar(years,distribution_tics_inac,color=\"cornflowerblue\",edgecolor=\"k\",hatch=\"/\",label=\"# of firms each year including inactive firms\")\n",
    "plt.bar(years,distribution_tics,color=\"indianred\",edgecolor=\"k\", label=\"# of firms each year with inactive firms dropped\")\n",
    "# Plot horisontal lines of mean # of unique firms\n",
    "plt.axhline(y = np.mean(distribution_tics_inac), linestyle = '--',color=\"k\",label=\"Mean # of firms per year, including inactive firms (\"+str(int(np.mean(distribution_tics_inac)))+\")\")\n",
    "plt.axhline(y = np.mean(distribution_tics), linestyle = '-',color=\"k\",label=\"Mean # of firms per year, with inactive firms dropped (\"+str(int(np.mean(distribution_tics)))+\")\")\n",
    "# Manipulate plot axis limits\n",
    "plt.xlim(1989,2023)\n",
    "plt.ylim(0,10000)\n",
    "# Define units for both x- and y axis\n",
    "years_3 = np.arange(1990,2023,2)\n",
    "plt.xticks(ticks=years_3, labels=years_3)\n",
    "y_ticks = [0,1000,2000,3000,4000,5000,6000,7000,8000]\n",
    "plt.yticks(ticks=y_ticks, labels=y_ticks)\n",
    "# Give plot a grid, and axis font-sizes\n",
    "plt.grid(axis=\"y\",alpha=0.5)\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=25)\n",
    "# The following bit of code changes the order that things are shown in the legend\n",
    "#----------------------------------\n",
    "#get handles and labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "#specify order of items in legend\n",
    "order = [2,3,0,1]\n",
    "#add legend to plot\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc=0,fontsize=30) \n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61895714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test train set\n",
    "from functools import reduce\n",
    "tics = np.unique(df[\"tic\"])\n",
    "N100 = pd.ExcelFile(\"Ticker NASDAQ100 data BLOOMBERG.xlsx\")\n",
    "Nasdaq100 = {}\n",
    "years = np.arange(2005,2023)\n",
    "Tickers = []\n",
    "nasdaq_total = []\n",
    "nasdaq_wehave = []\n",
    "for year in years:\n",
    "    Nasdaq100[year] = pd.read_excel(N100, str(year))\n",
    "    for j in range(len(Nasdaq100[year])):\n",
    "        Nasdaq100[year].at[j,\"Ticker\"] = Nasdaq100[year][\"Ticker\"].tolist()[j].split()[0]\n",
    "    nasdaq_total.append(len(Nasdaq100[year][\"Ticker\"]))\n",
    "    nasdaq_wehave.append(len(np.intersect1d(np.unique(df[\"tic\"]),Nasdaq100[year][\"Ticker\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "# Plot bar graphs of unique firm distributions\n",
    "plt.bar(years,nasdaq_total,color=\"cornflowerblue\",edgecolor=\"k\",hatch=\"/\",label=\"# of firms each year in Nasdaq 100\")\n",
    "plt.bar(years,nasdaq_wehave,color=\"indianred\",edgecolor=\"k\", label=\"# of firms each year from Nasdaq 100, that we have in dataset\")\n",
    "# Plot horisontal lines of mean # of unique firms\n",
    "plt.axhline(y = np.mean(nasdaq_total), linestyle = '--',color=\"k\",label=\"Mean # of firms per year in Nasdaq 100 (\"+str(int(np.mean(nasdaq_total)))+\")\")\n",
    "plt.axhline(y = np.mean(nasdaq_wehave), linestyle = '-',color=\"k\",label=\"Mean # of firms per year from Nasdaq 100, that we have in dataset (\"+str(int(np.mean(nasdaq_wehave)))+\")\")\n",
    "# Manipulate plot axis limits\n",
    "plt.xlim(2004,2023)\n",
    "plt.ylim(0,150)\n",
    "# Define units for both x- and y axis\n",
    "years_3 = np.arange(2005,2023,1)\n",
    "plt.xticks(ticks=years_3, labels=years_3)\n",
    "y_ticks = [0,20,40,60,80,100]\n",
    "plt.yticks(ticks=y_ticks, labels=y_ticks)\n",
    "# Give plot a grid, and axis font-sizes\n",
    "plt.grid(axis=\"y\",alpha=0.5)\n",
    "plt.rc('xtick', labelsize=25) \n",
    "plt.rc('ytick', labelsize=25)\n",
    "# The following bit of code changes the order that things are shown in the legend\n",
    "#----------------------------------\n",
    "#get handles and labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "#specify order of items in legend\n",
    "order = [2,3,0,1]\n",
    "#add legend to plot\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc=2,fontsize=30) \n",
    "#----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fce007",
   "metadata": {},
   "source": [
    "# Macro features plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41677fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fed_funds_raw = pd.read_csv(\"FEDFUNDS.csv\") ; Fed_funds_raw[\"datadate\"] = pd.to_datetime(Fed_funds_raw[\"DATE\"]) ; Fed_funds_raw = Fed_funds_raw.set_index(\"datadate\")\n",
    "GDP_raw = pd.read_csv(\"GDP.csv\") ; GDP_raw[\"datadate\"] = pd.to_datetime(GDP_raw[\"DATE\"]) ; GDP_raw = GDP_raw.set_index(\"datadate\")\n",
    "TB3MS_raw = pd.read_csv(\"TB3MS.csv\") ; TB3MS_raw[\"datadate\"] = pd.to_datetime(TB3MS_raw[\"DATE\"]) ; TB3MS_raw = TB3MS_raw.set_index(\"datadate\")\n",
    "DGS3_raw = pd.read_csv(\"DGS3.csv\") ; DGS3_raw[\"datadate\"] = pd.to_datetime(DGS3_raw[\"DATE\"]) ; DGS3_raw = DGS3_raw.set_index(\"datadate\")\n",
    "DGS10_raw = pd.read_csv(\"DGS10.csv\") ; DGS10_raw[\"datadate\"] = pd.to_datetime(DGS10_raw[\"DATE\"]) ; DGS10_raw = DGS10_raw.set_index(\"datadate\")\n",
    "UNRATE_raw = pd.read_csv(\"UNRATE.csv\"); UNRATE_raw[\"datadate\"] = pd.to_datetime(UNRATE_raw[\"DATE\"]) ; UNRATE_raw = UNRATE_raw.set_index(\"datadate\")\n",
    "\n",
    "Macro_df = pd.concat([Fed_funds_raw,GDP_raw[\"GDP_PCH\"],TB3MS_raw[\"TB3MS\"],DGS3_raw[\"DGS3\"],DGS10_raw[\"DGS10\"],UNRATE_raw[\"UNRATE\"]],axis=1)\n",
    "Macro_df = Macro_df[:-1] #Delete last row since this is empty\n",
    "Macro_df['DGS3'] = Macro_df['DGS3'].astype(float) #convert string to float\n",
    "Macro_df[\"DGS_diff\"] = Macro_df[\"DGS3\"]-Macro_df[\"DGS10\"]\n",
    "Macro_df[\"rf\"] = Macro_df[\"DGS10\"]\n",
    "Macro_df = Macro_df.drop([\"DGS3\",\"DGS10\"],axis=1)\n",
    "Macro_df = Macro_df.shift(-1)\n",
    "#Macro_df = Macro_df[:-1] #Delete last row since this is empty\n",
    "#Macro_df = Macro_df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf49604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import ConnectionPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f89d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "plt.plot(Macro_df[\"rf\"],label=\"rf\")\n",
    "plt.plot(Macro_df[\"DGS_diff\"],label=\"DGS_diff\",linewidth=2)\n",
    "plt.plot(Macro_df[\"FEDFUNDS\"],label=\"FEDFUNDS\",linewidth=2)\n",
    "plt.plot(Macro_df[\"TB3MS\"],label=\"TB3MS\",linewidth=2)\n",
    "plt.plot(Macro_df[\"GDP_PCH\"].dropna(),label=\"GDP_PCH\",linewidth=2)\n",
    "plt.plot(Macro_df[\"UNRATE\"],label=\"UNRATE\",linewidth=2)\n",
    "# Plot horisontal line on y=0\n",
    "plt.axhline(y = 0, color = 'k', linestyle = '--')\n",
    "# Define intervals for US recessions (from https://fredhelp.stlouisfed.org/fred/data/understanding-the-data/recession-bars/)\n",
    "# index is in days starting with 0 from 1970-01-01, no. of days dalculated from https://planetcalc.com/9788/\n",
    "rec_1 = [7485,7728] #rec_1: 1990-07-01, 1991-03-01\n",
    "rec_2 = [11381,11626] #rec_2: 2001-03-01, 2001-11-01\n",
    "rec_3 = [13847,14395] #rec_3: 2007-12-01, 2009-06-01\n",
    "rec_4 = [18292,18352] #rec_4: 2020-02-01, 2020-04-01\n",
    "plt.fill_between(rec_1, 0.17, -0.11, facecolor=(0,0,0,.2), edgecolor=(0,0,0),zorder=2) \n",
    "plt.fill_between(rec_2, 0.17, -0.11, facecolor=(0,0,0,.2), edgecolor=(0,0,0),zorder=2)\n",
    "plt.fill_between(rec_3, 0.17, -0.11, facecolor=(0,0,0,.2), edgecolor=(0,0,0),zorder=2)\n",
    "plt.fill_between(rec_4, 0.17, -0.11, facecolor=(0,0,0,.2), edgecolor=(0,0,0),zorder=2)\n",
    "\n",
    "plt.xlabel(\"Year\",fontsize=25)\n",
    "# Change what is shown on x-axis\n",
    "x_ticks = [7304,8400,9495,10591,11687,12783,13878,14974,16070,17166,18261,19357]\n",
    "years = np.arange(1990,2024,3)\n",
    "plt.xticks(ticks=x_ticks, labels=years)\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.grid(zorder=3)\n",
    "plt.ylim(-0.1,0.16)\n",
    "plt.xlim(7304,19388)\n",
    "plt.legend(loc=(0.05,0.7),fontsize=15)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "plt.plot(Macro_df[\"rf\"],label=\"rf\")\n",
    "plt.plot(Macro_df[\"DGS_diff\"],label=\"DGS_diff\",linewidth=2)\n",
    "plt.plot(Macro_df[\"FEDFUNDS\"],label=\"FEDFUNDS\",linewidth=2)\n",
    "plt.plot(Macro_df[\"TB3MS\"],label=\"TB3MS\",linewidth=2)\n",
    "plt.plot(Macro_df[\"GDP_PCH\"].dropna(),label=\"GDP_PCH\",linewidth=2)\n",
    "plt.plot(Macro_df[\"UNRATE\"],label=\"UNRATE\",linewidth=2)\n",
    "plt.axhline(y = 0, color = 'k', linestyle = '--')\n",
    "# Define intervals for US recessions (from https://fredhelp.stlouisfed.org/fred/data/understanding-the-data/recession-bars/)\n",
    "#rec_4: 2020-02-01, 2020-04-01\n",
    "# index is in days starting with 0 from 1970-01-01, # days dalculated from https://planetcalc.com/9788/\n",
    "rec_4 = [18292,18352]\n",
    "plt.fill_between(rec_4, 0.17, -0.11, facecolor=(0,0,0,.3), edgecolor=(0,0,0),zorder=2)\n",
    "\n",
    "#plt.title(\"Macroeconomic Features\",fontsize=36)\n",
    "plt.xlabel(\"Date\",fontsize=25)\n",
    "plt.grid(zorder=3)\n",
    "x_ticks = [18139,18200,18261,18321,18382,18443,18505]\n",
    "years = [\"2019-09\",\"2019-11\",\"2020-01\",\"2020-03\",\"2020-05\",\"2020-07\",\"2020-09\"]\n",
    "plt.xticks(ticks=x_ticks, labels=years)\n",
    "plt.rc('xtick', labelsize=15) \n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.ylim(-0.1,0.16)\n",
    "plt.xlim(18139,18505)\n",
    "plt.legend(loc=(0.05,0.6),fontsize=20)\n",
    "\n",
    "# Add line from one subplot to the other\n",
    "xyA = [18139, -0.101]\n",
    "xyB = [18139, 0.16]\n",
    "# ConnectionPatch handles the transform internally so no need to get fig.transFigure\n",
    "line = ConnectionPatch(\n",
    "    xyA,\n",
    "    xyB,\n",
    "    coordsA=ax1.transData,\n",
    "    coordsB=ax2.transData,\n",
    "    # Default shrink parameter is 0 so can be omitted\n",
    "    color=(0,0,0,1),\n",
    "    linestyle=\"--\",  # \"normal\" arrow\n",
    "    mutation_scale=10,  # controls arrow head size\n",
    "    linewidth=2,\n",
    ")\n",
    "fig.patches.append(line)\n",
    "\n",
    "# Add line from one subplot to the other\n",
    "xyA = [18505, -0.101]\n",
    "xyB = [18505, 0.16]\n",
    "# ConnectionPatch handles the transform internally so no need to get fig.transFigure\n",
    "line = ConnectionPatch(\n",
    "    xyA,\n",
    "    xyB,\n",
    "    coordsA=ax1.transData,\n",
    "    coordsB=ax2.transData,\n",
    "    # Default shrink parameter is 0 so can be omitted\n",
    "    color=(0,0,0,1),\n",
    "    linestyle=\"--\", # -|> \"normal\" arrow\n",
    "    mutation_scale=10,  # controls arrow head size\n",
    "    linewidth=2,\n",
    ")\n",
    "fig.patches.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16676131",
   "metadata": {},
   "source": [
    "# REST OF MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c357d",
   "metadata": {},
   "source": [
    "Since we decided that we wanted to compare our portfolio results to the Nasdaq 100 index, it makes sense that we build our portfolio on the very same Nasdaq 100 constituents. Thus, we create a test set only containing the firms available in our dataset that were/are Nasdaq constituents in the correct time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test train set\n",
    "from functools import reduce\n",
    "N100 = pd.ExcelFile(\"Ticker NASDAQ100 data BLOOMBERG.xlsx\") # Load in Nasdaq 100 constituents for every year between 2005 and 2023\n",
    "Nasdaq100 = {} # Create dictionary that will contain the constituents for each year\n",
    "# Divide out information from each excel sheet in dictionary \n",
    "years = np.arange(2004,2024)\n",
    "Tickers = []\n",
    "for year in years:\n",
    "    Nasdaq100[year] = pd.read_excel(N100, str(year))\n",
    "    for j in range(len(Nasdaq100[year])):\n",
    "        Nasdaq100[year].at[j,\"Ticker\"] = Nasdaq100[year][\"Ticker\"].tolist()[j].split()[0]\n",
    "    Tickers = reduce(np.union1d, (Tickers,Nasdaq100[year][\"Ticker\"])) # End up with complete list of Nasdaq 100 constituents\n",
    "    \n",
    "        \n",
    "unique_tics = np.unique(df[\"tic\"]) # Determine the unique tickers in dataset\n",
    "intersect = np.intersect1d(unique_tics,Tickers) # Find the intersection between tickers in dataset and Nasdaq 100 constituents \n",
    "train = df[~df[\"tic\"].isin(intersect)] # Create train set with every ticker NOT in Nasdaq 100\n",
    "test = df[df[\"tic\"].isin(intersect)] # Create test set with every ticker in Nasdaq 100\n",
    "test_pf = df[df[\"tic\"].isin(intersect)] # Create duplicate test set with every ticker in Nasdaq 100 for portfolio stuff\n",
    "train = train.drop([\"prccm\",\"mkt cap\"],axis = 1) # Remove close price and market cap, since these are not used for training \n",
    "test = test.drop([\"prccm\",\"mkt cap\"],axis = 1) # Remove close price and market cap, since these are not used for testing\n",
    "test_pf = test_pf[[\"datadate\",\"tic\",\"prccm\",\"mkt cap\"]] # Only keep datadate, ticker, close price, and market cap in duplicate test set used for portfolio stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale both train and test set\n",
    "for col in train.columns[2:-1]:\n",
    "    scaler = MinMaxScaler()\n",
    "    train[[col]] = scaler.fit_transform(train[[col]]) # Use fit_transform on training set\n",
    "    test[[col]] = scaler.transform(test[[col]]) # Use transform on test set. ONLY NASDAQ COMPANIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f70cf2",
   "metadata": {},
   "source": [
    "The next functions are what uses the input_size parameter from earlier. Basically, we want to represent the data in \"chunks\" of _input_size_ (this case 12) datapoints for each firm at a time. Thus, the first function creates a list, _dates_df_, of dataframes for the train set each containing a years worth of data with the next index of the list sliding one month ahead. The final output is as said a list where the index is the corresponding month's index (dates_df[0] contains the first month and so on)\n",
    "\n",
    "The bottom function does roughly the same but for the test set. A key difference is, that the resulting list is a level deeper since we need to keep track of specific tickers for each year. Thus the first index corresponds to the year and the next index is the month (dates_df_test[0][0] will be first month of first year, while dates_df_test[1][11] is the last month of the next year and so on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_size_func(df):\n",
    "    dates_df = [] # Allocate storage to save data for each month\n",
    "    unique_dates = np.unique(df[\"datadate\"]) # Determine the unique dates \n",
    "    for i in range(input_size-1,len(unique_dates)): # Iterate over every month but start at \"input_size\"-1 since we cant look 12 months back starting at month 0\n",
    "        try:\n",
    "            # Look at data in range of \"input_size\" sliding one month ahead at a time\n",
    "            dates_df.append(df[(df[\"datadate\"]>=unique_dates[i-input_size+1]) & (df[\"datadate\"]<=unique_dates[i])].reset_index(drop=True))\n",
    "        except:\n",
    "            None\n",
    "    return dates_df\n",
    "\n",
    "def input_size_func_test(dictionary,test_set):\n",
    "    input_size_test = [] # Allocate storage to save data for each month\n",
    "    years = np.arange(2019,2023) #'''REMEMBER TO CHANGE!!!''' # Decide which years to iterate over (three respective periods)\n",
    "    for year in years:\n",
    "        temp=test_set[test_set[\"tic\"].isin(np.unique(dictionary[year][\"Ticker\"]))] # Look at specific tickers every year        \n",
    "        temp_test=temp[(temp[\"datadate\"]>=str(year-1)+\"-02-01\") & (temp[\"datadate\"]<str(year+1) )] # Look at data in range of \"input_size\" sliding one month ahead at a time\n",
    "        input_size_test.append(input_size_func(temp_test)) # Use above function for each year, creating a list that will be 1 level deeper\n",
    "    return input_size_test\n",
    "        \n",
    "dates_df_train = input_size_func(train)    \n",
    "dates_df_test_dict = input_size_func_test(Nasdaq100,test)\n",
    "dates_df_test_pf = input_size_func_test(Nasdaq100,test_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c41730",
   "metadata": {},
   "source": [
    "We noticed a little too late on, that the way we created the _input_size_ data in the above functions, we did not make sure that we only had data in chunks of 12. Thus, some places would contain a chunk of some random number of data points. This created a bunch of issues, since we need chunks of 12. A quick fix was to create two functions that could remove the inconsistensies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_input_size_errors_train(dates_df_train):\n",
    "    for l in tqdm_notebook(range(len(dates_df_train))): # Iterate over every index of list \n",
    "        k=0 # Assign start-index variable, so we can start next loop from index where last item is removed\n",
    "        stop = 1 # Assign dummy variable to determine if we have removed something and can stop the current list index \n",
    "        # (stop == 0 means that we have not stopped in an entire run-through, and can then jump to next index) \n",
    "        for j in range(99999999):\n",
    "            if stop == 0: # If we have not stopped in the previous run-through of current index there are no inconsitensies, we can jump to next index\n",
    "                break\n",
    "            stop = 0 # Set stop variable\n",
    "            for i in range(k,len(dates_df_train[l]),input_size): # Jump 12 each step, start at k so we dont have to start from beginning when removing \n",
    "                try:\n",
    "                    if dates_df_train[l].iloc[i][\"tic\"]!=dates_df_train[l].iloc[i+input_size-1][\"tic\"]: # If the firm 11 places in front is different we dont have chunk of 12\n",
    "                        dates_df_train[l] = dates_df_train[l].drop(i).reset_index(drop=True) # Remove current index and reset index\n",
    "                        k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                        stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "                        break\n",
    "                except:\n",
    "                    dates_df_train[l] = dates_df_train[l].drop(i).reset_index(drop=True) # We will end up in except statement near the end if there is inconsistency. Remove this\n",
    "                    k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                    stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "    return dates_df_train\n",
    "\n",
    "def remove_input_size_errors_test(dates_df_test):\n",
    "    # Remember that this list is a level deeper, so we need to keep track of both year and month\n",
    "    year = 0\n",
    "    month = 0\n",
    "    for l in tqdm_notebook(range(len(dates_df_test)*12)): # The total number of indexes is the amount of years multiplied with 12\n",
    "        k=0 # Assign start-index variable, so we can start next loop from index where last item is removed\n",
    "        stop = 1 # Assign dummy variable to determine if we have removed something and can stop the current list index\n",
    "        for j in range(9999999):\n",
    "            if stop == 0: # If we have not stopped in the previous run-through of current index there are no inconsitensies, we can jump to next index\n",
    "                break\n",
    "            stop = 0 # Set stop variable\n",
    "            for i in range(k,len(dates_df_test[year][month]),input_size): # Jump 12 each step, start at k so we dont have to start from beginning when removing \n",
    "                try:\n",
    "                    if dates_df_test[year][month].iloc[i][\"tic\"]!=dates_df_test[year][month].iloc[i+input_size-1][\"tic\"]: # If the firm 11 places in front is different we dont have chunk of 12\n",
    "                        dates_df_test[year][month] = dates_df_test[year][month].drop(i).reset_index(drop=True) # Remove current index and reset index\n",
    "                        k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                        stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "                        break\n",
    "                except:\n",
    "                    dates_df_test[year][month] = dates_df_test[year][month].drop(i).reset_index(drop=True) # We will end up in except statement near the end if there is inconsistency. Remove this\n",
    "                    k = i # Set start-index, since the next inconsistency will always come after the one we just removed\n",
    "                    stop = 1 # Since we have removed datapoint, we have stopped, and thus it is not time to break out of loop since there might be more inconsitencies\n",
    "        month = (month+1) % 12 # Update month (every time we reach 12 it will reset to 0 and begin a new year)\n",
    "        year = int(np.floor((l+1)/12)) # Update year, only change when we reach 12th, 24th, ... index\n",
    "    return dates_df_test\n",
    "\n",
    "dates_df_train = remove_input_size_errors_train(dates_df_train)\n",
    "dates_df_test_dict = remove_input_size_errors_test(dates_df_test_dict)\n",
    "dates_df_test_pf = remove_input_size_errors_test(dates_df_test_pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c0e37",
   "metadata": {},
   "source": [
    "The next functions split datasets into features and target, but also keeps individual firms separated. We want the data in a way where 12 months of features correspond to the last month's target. Example: We save features for one firm from Jan to Dec, and save the target for Dec.\n",
    "\n",
    "Again, bottom function is a level deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dictionary):\n",
    "    # Create dictionaries that will contain features and target \n",
    "    X = {}\n",
    "    y = {}\n",
    "    for i in tqdm_notebook(range(len(dictionary))):\n",
    "        # Create temporary lists that will contain dataframes containg chunks of 12 for each individual firm, so 1 dataframe per firm\n",
    "        save_X = []\n",
    "        save_y = []\n",
    "        try:\n",
    "            for j in range(len(dictionary[i])):\n",
    "                # If next firm is different, we need to save the features from previous 11 months and from current month + target from current month\n",
    "                if dictionary[i][\"tic\"].iloc[j] != dictionary[i][\"tic\"].iloc[j+1]: \n",
    "                    save_X.append(dictionary[i].iloc[j-input_size+1:j+1,2:-1]) # Save features \n",
    "                    save_y.append(dictionary[i].iloc[j,-1]) # Save target\n",
    "                    \n",
    "        except: # We end up in the except statement in the end\n",
    "            save_X.append(dictionary[i].iloc[j-input_size+1:j+1,2:-1]) # Save features \n",
    "            save_y.append(dictionary[i].iloc[j,-1]) # Save target\n",
    "        #Save temporary lists in dictionary\n",
    "        X[i] = save_X \n",
    "        y[i] = save_y\n",
    "    return X,y\n",
    "\n",
    "def split_test(test_dict):\n",
    "    # Create dictionaries that will contain features and target \n",
    "    X_temp,y_temp = {},{}\n",
    "    for k in range(len(test_dict)):\n",
    "        X_,y_ = split(test_dict[k]) # Use above function for every year of test set\n",
    "        X_temp[k] = X_ # Save features for each year in dictionary\n",
    "        y_temp[k] = y_ # Save target for each year in dictionary\n",
    "    return X_temp,y_temp\n",
    "\n",
    "X_train, y_train = split(dates_df_train)\n",
    "X_test, y_test = split_test(dates_df_test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655239e3",
   "metadata": {},
   "source": [
    "We are now done with data manipulation, and ready to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model is not run from beginning, but pickled data is loaded in, run this to assign varaibles\n",
    "input_size = 12\n",
    "n_features = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN model that we use in project\n",
    "def create_model_rnn(neurons,learning_rate,drop_out,hidden):\n",
    "    early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50) # Stop if no progress in # epochs\n",
    "    #checkpoint = tf.keras.callbacks.ModelCheckpoint(\"weights.best.hdf5\",monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [early] #,checkpoint\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    model.add(Dropout(rate=drop_out))\n",
    "    \n",
    "    # Create functionality for using different amounts of hidden layers used in grid search\n",
    "    if hidden == 1:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    elif  hidden == 2:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    elif  hidden == 3:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "    elif  hidden == 4:\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        model.add(SimpleRNN(units = neurons, activation = 'relu', input_shape=(input_size, n_features), return_sequences=True))\n",
    "        \n",
    "    model.add(SimpleRNN(units = neurons, activation = 'relu', return_sequences=False ))\n",
    "    model.add(Dense(units = 1)) #Linear output layer\n",
    "    opt = optimizers.Adam(lr=learning_rate, clipnorm=1.)  \n",
    "    model.compile(optimizer = opt, loss = \"mse\")\n",
    "    return model,callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4db3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "neurons = [50]\n",
    "epochs = [50]\n",
    "learning_rate = [0.0005,0.001] \n",
    "drop_out = [0.05,0.01]\n",
    "batch_size = [32,64]\n",
    "hidden = [1,2]\n",
    "val_size = 0.2\n",
    "loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f82a4",
   "metadata": {},
   "source": [
    "Below, we do the grid search. We run a model for every combination of hyperparameters given above and sort the different models on their validation mse.\n",
    "\n",
    "Bottom function makes it possible to run big grid on firstt month and then use optimal hyperparameters on next 2 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_model(X_train,y_train,starting_point,end_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out):\n",
    "    optimal = []\n",
    "    if len(neurons) == 1 & len(epochs) == 1 & len(learning_rate) == 1 & len(drop_out) == 1 & len(batch_size) == 1 & starting_point == end_point:\n",
    "        verbose = 1\n",
    "    else:\n",
    "        verbose = 0\n",
    "    for i in tqdm_notebook(range(starting_point,end_point+1)):\n",
    "        print(i)\n",
    "        print(\"---\")\n",
    "        temp = []\n",
    "        for j in range(len(neurons)):\n",
    "            for q in range(len(epochs)):\n",
    "                for k in range(len(learning_rate)):\n",
    "                    for p in range(len(drop_out)):\n",
    "                        for l in range(len(batch_size)):\n",
    "                            for h in range(len(hidden)):\n",
    "                                print(\"Neurons:\",neurons[j],\". Epochs:\",epochs[q],\". Learning rate:\",learning_rate[k],\". Dropout:\",drop_out[p],\". Batch size:\",batch_size[l], \". Hidden layers:\", hidden[h])\n",
    "                                # Create model using function above\n",
    "                                model,callbacks_list = create_model_rnn(neurons[j],learning_rate[k],drop_out[p],hidden[h])\n",
    "                                # Fit model on training data, save loss history\n",
    "                                hist = model.fit(np.array(X_train[i]),np.array(y_train[i]),epochs=epochs[q], verbose=verbose,callbacks=callbacks_list,validation_split=0.2,batch_size = batch_size[l]) \n",
    "                                # Predict data using train as input to calculate train MSE\n",
    "                                pred_train = model.predict(np.array(X_train[i]))\n",
    "                                # Predict data using validation data as input to calculate validation MSE\n",
    "                                pred_val = model.predict(np.array(X_train[i])[int(len(np.array(X_train[i]))*0.8):])\n",
    "                                # Save dataframe of different MSEs, hyperparameters used, models, and loss results\n",
    "                                temp.append([mean_squared_error(np.array(y_train[i]),pred_train),mean_squared_error(np.array(y_train[i])[int(len(y_train[i])*0.8):],pred_val),neurons[j],epochs[q],learning_rate[k],drop_out[p],batch_size[l],hidden[h],model,hist.history])\n",
    "        df_temp = pd.DataFrame(data = temp,columns = (\"train mse\",\"val mse\",\"neurons\",\"epochs\",\"learning_rate\",\"dropout\",\"batch size\",\"hidden layers\",\"model_save\",\"loss_hist\"))\n",
    "        optimal.append(df_temp)\n",
    "    return optimal, starting_point\n",
    "\n",
    "def run_multiple(X_train,y_train,starting_point,end_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out):\n",
    "    optimal_1, index_start = find_optimal_model(X_train,y_train,starting_point,starting_point,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "    temp_hyper = optimal_1[0][optimal_1[0][\"val mse\"]==(optimal_1[0][\"val mse\"].min())]\n",
    "    optimal_2, start = find_optimal_model(X_train,y_train,starting_point+1,end_point,[temp_hyper.iloc[0][\"epochs\"]],[temp_hyper.iloc[0][\"neurons\"]],[temp_hyper.iloc[0][\"learning_rate\"]],[temp_hyper.iloc[0][\"batch size\"]],[temp_hyper.iloc[0][\"hidden layers\"]],[temp_hyper.iloc[0][\"dropout\"]])\n",
    "    return optimal_1+optimal_2,index_start, end_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run above functions to find models, specify start and end index\n",
    "start = 0\n",
    "ending = 6 \n",
    "optimal1,index_start,index_end = run_multiple(X_train,y_train,start,start+2,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "optimal2,index_start,slut = run_multiple(X_train,y_train,index_end+1,ending,epochs,neurons,learning_rate,batch_size,hidden,drop_out)\n",
    "optimal = optimal1+optimal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find every optimal model and save in dataframe along with hyperparameters used\n",
    "def choose_optimal(optimal,index_start):\n",
    "    df = pd.DataFrame()\n",
    "    year = int(np.floor(index_start/12))\n",
    "    month = index_start % 12\n",
    "    for i in range(len(optimal)):\n",
    "        temp = optimal[i][optimal[i][\"val mse\"]==(optimal[i][\"val mse\"].min())]\n",
    "        df = df.append(temp)\n",
    "        print(year,month)\n",
    "        month = month+1\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "optimal_df = choose_optimal(optimal,start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cfdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimal_and_predict(optimal,X_test,index_start):\n",
    "    df = pd.DataFrame()\n",
    "    year = int(np.floor(index_start/12))\n",
    "    month = index_start % 12\n",
    "    \n",
    "    for i in range(len(optimal)):\n",
    "        temp = optimal[i][optimal[i][\"val mse\"]==(optimal[i][\"val mse\"].min())]\n",
    "        \n",
    "#         if i %12 == 0:\n",
    "#             year = year+1\n",
    "#             month = 0 #reset month if we change year\n",
    "        print(year,month)\n",
    "        temp_pred = temp.iloc[0][8].predict(np.array(X_test[year][month])) #[0][8] chooses the model in this line.\n",
    "        temp[\"pred\"] = [temp_pred]\n",
    "        df = df.append(temp)\n",
    "        month = month+1\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "optimal_df = choose_optimal_and_predict(optimal,X_test,start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "383.844px",
    "left": "1215px",
    "right": "20px",
    "top": "102px",
    "width": "631px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0600909b957fd4144ffd2201940c226316954f31b5752942d3f7c0d5648b359f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
